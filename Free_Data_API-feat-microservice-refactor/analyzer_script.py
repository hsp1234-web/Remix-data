# -*- coding: utf-8 -*-
# @title ğŸš€ å…¨æ™¯å¸‚å ´åˆ†æå„€ v1.6 - å¼·åˆ¶é‡çµ„èˆ‡æ¸¬è©¦å¹³å°
# @markdown ### ç³»çµ±ä»‹ç´¹
# @markdown æœ¬è…³æœ¬æ˜¯ç‚ºã€Œå…¨æ™¯å¸‚å ´åˆ†æå„€ã€é‡èº«å®šåšçš„**æœ¬åœ°åŒ–**è‡ªå‹•éƒ¨ç½²èˆ‡æ¸¬è©¦å¹³å°ã€‚å®ƒå°ˆæ³¨æ–¼åœ¨ä¸ä¾è³´ä»»ä½•å¤–éƒ¨å„²å­˜ï¼ˆå¦‚ Google Driveï¼‰çš„æƒ…æ³ä¸‹ï¼Œå¿«é€Ÿé©—è­‰æ ¸å¿ƒç¨‹å¼ç¢¼çš„é‚è¼¯æ­£ç¢ºæ€§ã€‚
# @markdown - **æœ¬åœ°å„ªå…ˆ**: æ‰€æœ‰æ•¸æ“šåº«èˆ‡å¿«å–æª”æ¡ˆå‡åœ¨ Colab çš„é«˜é€Ÿæœ¬åœ°è‡¨æ™‚ç›®éŒ„ `/content/temp_data` ä¸­ç”Ÿæˆèˆ‡è®€å¯«ã€‚
# @markdown - **ä¸€éµåŸ·è¡Œ**: è‡ªå‹•å®Œæˆéƒ¨ç½²ã€å®‰è£ã€åŸ·è¡Œèˆ‡é©—è­‰çš„å…¨éç¨‹ã€‚
# @markdown - **ç²¾æº–éƒ¨ç½²**: å·²æ ¹æ“šæ‚¨çš„æŒ‡ç¤ºï¼Œé è¨­éƒ¨ç½²æ‚¨æŒ‡å®šçš„ **`feat/initial-project-structure`** åˆ†æ”¯ã€‚
# @markdown ---
# @markdown ### v1.6 é–‹ç™¼æ—¥èªŒ (å¼·åˆ¶é‡çµ„)
# @markdown - **æ–°å¢ã€Œå¼·åˆ¶é‡çµ„ã€æ­¥é©Ÿ**: åœ¨ `git clone` ä¹‹å¾Œï¼Œè…³æœ¬æœƒè‡ªå‹•æª¢æ¸¬ã€Œé›™å±¤åŒ…è£¹ã€çµæ§‹ï¼Œä¸¦å°‡å…§å±¤å°ˆæ¡ˆçš„æ‰€æœ‰å…§å®¹ç§»å‹•åˆ°é ‚å±¤ï¼Œå¾æ ¹æœ¬ä¸Šè§£æ±º `ModuleNotFoundError`ã€‚
# @markdown - **é©æ‡‰æ€§å¢å¼·**: æ­¤è…³æœ¬ç¾åœ¨èƒ½å¤ è™•ç†æ‚¨ç•¶å‰åˆ†æ”¯çš„æ··äº‚çµæ§‹ã€‚

# ==============================================================================
# @markdown ### æ­¥é©Ÿ 1: ğŸ¯ ç¢ºèªéƒ¨ç½²ç›®æ¨™ (åƒæ•¸å·²ç‚ºæ‚¨è¨­å®š)
# ==============================================================================
# --- Git éƒ¨ç½²ç›¸é—œè¨­å®š ---
GITHUB_REPO_URL = "https://github.com/hsp1234-web/Free_Data_API.git" #@param {type:"string"}
BRANCH_NAME = "feat/initial-project-structure" #@param {type:"string"}
# ä¿®æ”¹è·¯å¾‘åˆ° /tmp
OUTER_CLONE_PATH = "/tmp/panoramic-market-analyzer" #@param {type:"string"}


# --- æ•¸æ“šç®¡é“æ¸¬è©¦åƒæ•¸ ---
TEST_SYMBOLS = "SPY,TLT,GLD,BTC-USD"  # @param {type:"string"}
TEST_START_DATE = "2023-01-01" # @param {type:"string"}
TEST_END_DATE = "2023-12-31"   # @param {type:"string"}

# --- æœ¬åœ°è‡¨æ™‚æ•¸æ“šè·¯å¾‘è¨­å®š ---
# ä¿®æ”¹è·¯å¾‘åˆ° /tmp
LOCAL_DATA_PATH = "/tmp/temp_data" #@param {type:"string"}

# ==============================================================================
# @markdown ### æ­¥é©Ÿ 2: â–¶ï¸ é»æ“ŠåŸ·è¡Œæ­¤å„²å­˜æ ¼
# ==============================================================================

# --- æ ¸å¿ƒå‡½å¼åº«å°å…¥èˆ‡è‡ªå‹•å®‰è£ ---
import os
import sys
import subprocess
import shutil
import io
import time
import threading
import logging
from datetime import datetime

# --- ä¾è³´å®‰è£ ---
try:
    import psutil
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "psutil"])
    import psutil
try:
    import pytz
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "pytz"])
    import pytz
try:
    import duckdb
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "duckdb"])
    import duckdb

try:
    from google.colab import drive, files
    from IPython.display import display, HTML
    IS_COLAB = True
except ImportError:
    IS_COLAB = False
    display, HTML = lambda x: print(str(x)), lambda x: x # type: ignore
    class MockFiles:
        def download(self, path): print(f"ä¸‹è¼‰æ¨¡æ“¬: è«‹æ‰‹å‹•å¾ {path} ç²å–æª”æ¡ˆã€‚")
    files = MockFiles() # type: ignore

# --- å…¨åŸŸè¨­å®šèˆ‡æ—¥èªŒç³»çµ± ---
TAIPEI_TZ = pytz.timezone('Asia/Taipei')
LOG_STREAM = io.StringIO()

class ColabLogger:
    def __init__(self, log_stream: io.StringIO):
        self.log_stream = log_stream
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.StreamHandler(self.log_stream)
            ]
        )
        self.logger = logging.getLogger("PlatformLogger")

    def info(self, msg, **kwargs): self.logger.info(f"âšª {msg}", **kwargs)
    def success(self, msg, **kwargs): self.logger.info(f"âœ… {msg}", **kwargs)
    def warning(self, msg, **kwargs): self.logger.warning(f"âš ï¸ {msg}", **kwargs)
    def error(self, msg, **kwargs): self.logger.error(f"âŒ {msg}", **kwargs)
    def header(self, msg):
        plain_header = f"\n{'='*80}\n=== {msg.strip()} ===\n{'='*80}"
        self.logger.info(plain_header)
        if IS_COLAB:
            display(HTML(f'<h3 style="color:black; border-bottom:2px solid #64B5F6; padding-bottom:5px; font-family:sans-serif; margin-top:1em;">{msg}</h3>'))

    def hw_log(self, msg): self.logger.info(f"â±ï¸ {msg}")
    def get_full_log(self): return self.log_stream.getvalue()

# --- ç¡¬é«”ç›£æ§ ---
class HardwareMonitor:
    def __init__(self, logger: ColabLogger):
        self.logger = logger
        self.stop_event = threading.Event()
        self.thread = None
        self.total_ram_gb = psutil.virtual_memory().total / (1024**3)

    def _monitor(self):
        while not self.stop_event.is_set():
            cpu_percent = psutil.cpu_percent(interval=1)
            ram = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            status = f"CPU: {cpu_percent:.1f}% | RAM: {ram.percent:.1f}% ({ram.used/(1024**3):.2f}/{self.total_ram_gb:.2f} GB) | Disk: {disk.percent:.1f}%"
            self.logger.hw_log(status)
            time.sleep(15)

    def start(self):
        self.thread = threading.Thread(target=self._monitor, daemon=True)
        self.thread.start()

    def stop(self):
        self.stop_event.set()
        if self.thread:
            self.thread.join(timeout=3)

# --- ä¸»åŸ·è¡Œæµç¨‹ ---
def run_deployment_and_test():
    start_time = time.time()
    logger = ColabLogger(LOG_STREAM)
    hw_monitor = HardwareMonitor(logger)

    if IS_COLAB:
        from IPython.display import clear_output
        clear_output(wait=True)

    logger.header("ğŸš€ å…¨æ™¯å¸‚å ´åˆ†æå„€ v1.6 - å¼·åˆ¶é‡çµ„èˆ‡æ¸¬è©¦å¹³å°å•Ÿå‹•")
    hw_monitor.start()

    # === éšæ®µé›¶: ç’°å¢ƒæº–å‚™ (æœ¬åœ°æ¨¡å¼) ===
    logger.header("éšæ®µé›¶: ç’°å¢ƒæº–å‚™ (æœ¬åœ°æ¨¡å¼)")
    logger.warning("Google Drive æœªæ›è¼‰ã€‚æ‰€æœ‰æ•¸æ“šå°‡å¯«å…¥ Colab æœ¬åœ°è‡¨æ™‚ç›®éŒ„ï¼Œæœƒè©±çµæŸå¾Œå°‡è¢«æ¸…é™¤ã€‚")
    if not os.path.exists(LOCAL_DATA_PATH):
        os.makedirs(LOCAL_DATA_PATH)
        logger.info(f"å·²å‰µå»ºæœ¬åœ°è‡¨æ™‚æ•¸æ“šç›®éŒ„: {LOCAL_DATA_PATH}")
    else:
        logger.info(f"æœ¬åœ°è‡¨æ™‚æ•¸æ“šç›®éŒ„å·²å­˜åœ¨: {LOCAL_DATA_PATH}")


    # === éšæ®µä¸€: Git å°ˆæ¡ˆéƒ¨ç½² ===
    logger.header("éšæ®µä¸€: Git å°ˆæ¡ˆéƒ¨ç½²")
    # ä¿®æ”¹åŸºç¤éƒ¨ç½²ç›®éŒ„åˆ° /tmp
    base_deployment_path = "/tmp"
    os.chdir(base_deployment_path)
    logger.info(f"åˆ‡æ›åˆ°åŸºç¤éƒ¨ç½²ç›®éŒ„: {base_deployment_path}")

    if os.path.exists(OUTER_CLONE_PATH):
        logger.info(f"åµæ¸¬åˆ°èˆŠç›®éŒ„ï¼Œæ­£åœ¨æ¸…ç† '{OUTER_CLONE_PATH}'...")
        shutil.rmtree(OUTER_CLONE_PATH)

    git_command = ["git", "clone", "--branch", BRANCH_NAME, "--single-branch", GITHUB_REPO_URL, OUTER_CLONE_PATH]
    logger.info(f"åŸ·è¡ŒæŒ‡ä»¤: {' '.join(git_command)}")
    git_result = subprocess.run(git_command, capture_output=True, text=True, encoding='utf-8')

    if git_result.returncode != 0:
        logger.error(f"âŒ Git clone å¤±æ•—ï¼\n{git_result.stderr.strip()}", exc_info=False)
        hw_monitor.stop()
        return
    else:
        logger.success("âœ… å°ˆæ¡ˆåˆ†æ”¯æˆåŠŸæ‹‰å–è‡³æœ¬åœ°ï¼")

    # === éšæ®µäºŒ: å¼·åˆ¶é‡çµ„èˆ‡è·¯å¾‘è¨­ç½® ===
    logger.header("éšæ®µäºŒ: å¼·åˆ¶é‡çµ„èˆ‡è·¯å¾‘è¨­ç½®")

    inner_project_path = os.path.join(OUTER_CLONE_PATH, "panoramic-market-analyzer")

    if os.path.isdir(inner_project_path):
        logger.warning(f"åµæ¸¬åˆ°é›™å±¤åŒ…è£¹çµæ§‹ï¼å…§å±¤è·¯å¾‘: {inner_project_path}ã€‚æ­£åœ¨åŸ·è¡Œå¼·åˆ¶é‡çµ„...")
        items_to_move = os.listdir(inner_project_path)
        logger.info(f"æº–å‚™å¾ '{inner_project_path}' ç§»å‹•ä»¥ä¸‹é …ç›®åˆ° '{OUTER_CLONE_PATH}': {items_to_move}")

        all_moved_successfully = True
        for item in items_to_move:
            source_item_path = os.path.join(inner_project_path, item)
            destination_item_path_final = os.path.join(OUTER_CLONE_PATH, item)

            logger.info(f"æº–å‚™ç§»å‹• '{source_item_path}' -> '{destination_item_path_final}'")

            try:
                if item == "data_pipeline" and os.path.isdir(destination_item_path_final):
                    logger.warning(f"ç›®æ¨™ç›®éŒ„ '{destination_item_path_final}' å·²å­˜åœ¨ã€‚æ­£åœ¨åˆªé™¤ä»¥ç¢ºä¿æ­£ç¢ºè¦†è“‹...")
                    shutil.rmtree(destination_item_path_final)
                    logger.success(f"âœ… å·²åˆªé™¤èˆŠçš„ '{destination_item_path_final}'ã€‚")

                shutil.move(source_item_path, destination_item_path_final)
                logger.info(f"âœ… æˆåŠŸç§»å‹• '{source_item_path}' -> '{destination_item_path_final}'")
            except Exception as e:
                logger.error(f"âŒ ç§»å‹• '{source_item_path}' åˆ° '{destination_item_path_final}' å¤±æ•—: {e}", exc_info=True)
                all_moved_successfully = False

        if all_moved_successfully:
            logger.success(f"âœ… æ‰€æœ‰é …ç›®å·²æˆåŠŸå¾ '{inner_project_path}' ç§»å‹•åˆ° '{OUTER_CLONE_PATH}'ã€‚")
            try:
                os.rmdir(inner_project_path)
                logger.success(f"âœ… ç©ºçš„å…§å±¤è³‡æ–™å¤¾ '{inner_project_path}' å·²æˆåŠŸç§»é™¤ã€‚")
            except Exception as e:
                logger.error(f"âŒ ç§»é™¤ç©ºçš„å…§å±¤è³‡æ–™å¤¾ '{inner_project_path}' å¤±æ•—: {e}", exc_info=True)
        else:
            logger.error("âŒ å¼·åˆ¶é‡çµ„éç¨‹ä¸­éƒ¨åˆ†é …ç›®ç§»å‹•å¤±æ•—ã€‚è«‹æª¢æŸ¥ä»¥ä¸Šæ—¥èªŒã€‚")

        expected_config_path = os.path.join(OUTER_CLONE_PATH, 'config.yaml')
        expected_commander_path = os.path.join(OUTER_CLONE_PATH, 'data_pipeline', 'commander.py')

        if os.path.exists(expected_config_path):
            logger.success(f"âœ… é©—è­‰æˆåŠŸ: é—œéµæ–‡ä»¶ 'config.yaml' å­˜åœ¨æ–¼ '{OUTER_CLONE_PATH}'ã€‚")
        else:
            logger.error(f"âŒ é©—è­‰å¤±æ•—: é—œéµæ–‡ä»¶ 'config.yaml' æœªåœ¨ '{OUTER_CLONE_PATH}' ä¸­æ‰¾åˆ°ã€‚")

        if os.path.exists(expected_commander_path):
            logger.success(f"âœ… é©—è­‰æˆåŠŸ: é—œéµæ¨¡çµ„ 'data_pipeline/commander.py' å­˜åœ¨æ–¼ '{OUTER_CLONE_PATH}'ã€‚")
        else:
            logger.error(f"âŒ é©—è­‰å¤±æ•—: é—œéµæ¨¡çµ„ 'data_pipeline/commander.py' æœªåœ¨ '{OUTER_CLONE_PATH}' ä¸­æ‰¾åˆ°ã€‚")
            logger.info(f"è«‹æª¢æŸ¥ '{os.path.join(OUTER_CLONE_PATH, 'data_pipeline')}' ç›®éŒ„çš„å…§å®¹ã€‚")
            if os.path.exists(os.path.join(OUTER_CLONE_PATH, 'data_pipeline')):
                 logger.info(f"'{os.path.join(OUTER_CLONE_PATH, 'data_pipeline')}' ç›®éŒ„å…§å®¹: {os.listdir(os.path.join(OUTER_CLONE_PATH, 'data_pipeline'))}")
            else:
                logger.warning(f"'{os.path.join(OUTER_CLONE_PATH, 'data_pipeline')}' ç›®éŒ„ä¸å­˜åœ¨ã€‚")
    else:
        logger.info(f"æœªåµæ¸¬åˆ°é›™å±¤åŒ…è£¹çµæ§‹æ–¼ '{inner_project_path}'ã€‚è·³éå¼·åˆ¶é‡çµ„ã€‚")
        expected_commander_path = os.path.join(OUTER_CLONE_PATH, 'data_pipeline', 'commander.py')
        if not os.path.exists(expected_commander_path):
            logger.warning(f"æ³¨æ„: æœªåŸ·è¡Œé‡çµ„ï¼Œä¸” 'data_pipeline/commander.py' æœªåœ¨ '{OUTER_CLONE_PATH}' ä¸­æ‰¾åˆ°ã€‚")

    # === éšæ®µ 2.5: ä½¿ç”¨ä¿®æ­£å¾Œçš„ç‰ˆæœ¬è¦†å¯« YFinanceFetcher.py ===
    logger.header("éšæ®µ 2.5: ä½¿ç”¨ä¿®æ­£å¾Œçš„ç‰ˆæœ¬è¦†å¯« YFinanceFetcher.py")
    yfinance_fetcher_path = os.path.join(OUTER_CLONE_PATH, "data_pipeline", "fetchers", "yfinance_fetcher.py")

    correct_yfinance_fetcher_content = """\
# fetchers/yfinance_fetcher.py
# Modified to include more robust MultiIndex and date column handling
import yfinance as yf
import pandas as pd
import time
import random
import logging
from typing import Optional
from ..interfaces.data_fetcher_interface import DataFetcherInterface

class YFinanceFetcher(DataFetcherInterface):
    \"\"\"ä½¿ç”¨ yfinance ç²å–é‡‘èæ•¸æ“šçš„ç©©å¥å¯¦ç¾ã€‚\"\"\"

    def __init__(self, robustness_config: dict):
        self.config = robustness_config
        self.logger = logging.getLogger(self.__class__.__name__)

    def fetch(self, symbol: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        current_delay = self.config['delay_min_seconds']
        for attempt in range(self.config['retries']):
            try:
                self.logger.info(f"Attempt {{attempt + 1}}/{{self.config['retries']}} to fetch {{symbol}} from yfinance...")
                # 1. æ˜ç¢ºè¨­å®š auto_adjust=False
                data = yf.download(symbol, start=start_date, end=end_date, progress=False, auto_adjust=False)

                # 2. æ·»åŠ æœ€ç°¡èª¿è©¦æ—¥èªŒ
                self.logger.info(f"YF_DEBUG Symbol passed to yf.download: {{symbol}}")
                if not data.empty:
                    self.logger.info(f"YF_DEBUG Raw data columns for {{symbol}}: {{data.columns.tolist()}}")
                    self.logger.info(f"YF_DEBUG Raw data index for {{symbol}}: {{data.index}}")
                    self.logger.info(f"YF_DEBUG Raw data head for {{symbol}}:\\n{{data.head().to_string()}}")
                else:
                    self.logger.info(f"YF_DEBUG Raw data for {{symbol}} is empty.")

                if data.empty:
                    self.logger.warning(f"No data found for {{symbol}} on yfinance for the given period: {{start_date}} to {{end_date}} (with auto_adjust=False).")
                    return None

                # æ¨™æº–åŒ–æ•¸æ“šæ ¼å¼
                data.reset_index(inplace=True)

                # 3. è™•ç† MultiIndex åˆ—
                if isinstance(data.columns, pd.MultiIndex):
                    self.logger.info(f"YF_DEBUG Detected MultiIndex columns for {{symbol}}. Attempting to flatten.")
                    new_columns = []
                    for col_tuple in data.columns:
                        if isinstance(col_tuple, tuple):
                            # å„ªå…ˆå–å…ƒçµ„çš„ç¬¬ä¸€å€‹å…ƒç´ ï¼Œå¦‚æœç‚ºç©ºå‰‡å–ç¬¬äºŒå€‹ (é©ç”¨æ–¼ ('Date', '') çš„æƒ…æ³)
                            new_columns.append(col_tuple[0] if col_tuple[0] else col_tuple[1])
                        else:
                            new_columns.append(col_tuple)
                    data.columns = new_columns
                    self.logger.info(f"YF_DEBUG Columns after flattening MultiIndex for {{symbol}}: {{data.columns.tolist()}}")
                    self.logger.info(f"YF_DEBUG Data head after flattening MultiIndex for {{symbol}}:\\n{{data.head().to_string()}}")

                # å°‡æ‰€æœ‰åˆ—åè½‰ç‚ºå°å¯«ä¸¦æ›¿æ›ç©ºæ ¼ç‚ºä¸‹åŠƒç·š
                data.columns = [str(col).lower().replace(' ', '_') for col in data.columns]

                # ç¢ºä¿ 'date' åˆ—æ˜¯ä¸»è¦çš„æ—¥æœŸæ™‚é–“åˆ—
                if 'date' not in data.columns:
                    if 'index' in data.columns:
                        self.logger.info(f"YF_DEBUG Renaming 'index' column to 'date' for {{symbol}}.")
                        data.rename(columns={{'index': 'date'}}, inplace=True)
                    elif 'datetime' in data.columns:
                        self.logger.info(f"YF_DEBUG Renaming 'datetime' column to 'date' for {{symbol}}.")
                        data.rename(columns={{'datetime': 'date'}}, inplace=True)

                expected_cols = ['open', 'high', 'low', 'close', 'adj_close', 'volume']
                for col in expected_cols:
                    if col not in data.columns:
                        self.logger.warning(f"Column '{{col}}' not found in standardized data for {{symbol}}. Filling with NaN.")
                        data[col] = pd.NA

                if 'date' in data.columns:
                    data['date'] = pd.to_datetime(data['date'])
                else:
                    self.logger.error(f"Critical: 'date' column still not found after all standardization attempts for {{symbol}}. Columns: {{data.columns.tolist()}}")
                    return None

                return data

            except Exception as e:
                self.logger.error(f"Error fetching {{symbol}} from yfinance on attempt {{attempt + 1}}: {{e}}", exc_info=True)
                if attempt < self.config['retries'] - 1:
                    sleep_time = current_delay + random.uniform(0, current_delay * 0.1)
                    self.logger.info(f"Waiting {{sleep_time:.2f}} seconds before retrying...")
                    time.sleep(sleep_time)
                    current_delay = min(current_delay * self.config.get('backoff_factor', 2), self.config['delay_max_seconds'])
                else:
                    self.logger.critical(f"Failed to fetch {{symbol}} from yfinance after all {{self.config['retries']}} retries.")
                    return None
        return None
"""
    # Correctly prepare the content for writing (unescape f-string like parts for the target file)
    final_yfinance_fetcher_code = correct_yfinance_fetcher_content.replace("{{symbol}}", "{symbol}") \
                                                               .replace("{{attempt + 1}}", "{attempt + 1}") \
                                                               .replace("{{self.config['retries']}}", "{self.config['retries']}") \
                                                               .replace("{{start_date}}", "{start_date}") \
                                                               .replace("{{end_date}}", "{end_date}") \
                                                               .replace("{{data.columns.tolist()}}", "{data.columns.tolist()}") \
                                                               .replace("{{data.index}}", "{data.index}") \
                                                               .replace("{{data.head().to_string()}}", "{data.head().to_string()}") \
                                                               .replace("{{col}}", "{col}") \
                                                               .replace("{{sleep_time:.2f}}", "{sleep_time:.2f}") \
                                                               .replace("{{e}}", "{e}")

    if not os.path.isdir(OUTER_CLONE_PATH):
        logger.error(f"âŒ å…‹éš†ç›®éŒ„ '{OUTER_CLONE_PATH}' ä¸å­˜åœ¨ã€‚ç„¡æ³•ç¹¼çºŒã€‚")
        hw_monitor.stop()
        return

    os.chdir(OUTER_CLONE_PATH) # åˆ‡æ›åˆ°å…‹éš†ä¸‹ä¾†çš„ç›®éŒ„
    sys.path.insert(0, OUTER_CLONE_PATH) # å°‡å°ˆæ¡ˆè·¯å¾‘åŠ å…¥ sys.path

    # ç¢ºä¿ fetchers ç›®éŒ„å­˜åœ¨
    fetchers_dir = os.path.join(OUTER_CLONE_PATH, "data_pipeline", "fetchers")
    if not os.path.exists(fetchers_dir):
        os.makedirs(fetchers_dir)
        logger.info(f"å·²å‰µå»ºç›®éŒ„: {fetchers_dir}")

    if os.path.exists(os.path.dirname(yfinance_fetcher_path)):
        try:
            with open(yfinance_fetcher_path, 'w', encoding='utf-8') as f:
                f.write(final_yfinance_fetcher_code)
            logger.success(f"âœ… '{yfinance_fetcher_path}' å·²è¢«ä¿®æ­£å¾Œçš„ç‰ˆæœ¬æˆåŠŸè¦†å¯«ã€‚")
        except Exception as e:
            logger.error(f"âŒ è¦†å¯« '{yfinance_fetcher_path}' å¤±æ•—: {e}", exc_info=True)
    else:
        logger.error(f"âŒ ç›®éŒ„ '{os.path.dirname(yfinance_fetcher_path)}' ä¸å­˜åœ¨ã€‚ç„¡æ³•è¦†å¯« YFinanceFetcherã€‚")

    # === éšæ®µ 2.6: ä¿®æ”¹ config.yaml ä»¥è¨­å®š max_workers = 1 ===
    logger.header("éšæ®µ 2.6: ä¿®æ”¹ config.yaml ä»¥è¨­å®š max_workers = 1")
    config_yaml_path = os.path.join(OUTER_CLONE_PATH, "config.yaml")
    if os.path.exists(config_yaml_path):
        try:
            import yaml # ç¢ºä¿å°å…¥ yaml
            with open(config_yaml_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)

            if 'concurrency' not in config_data:
                config_data['concurrency'] = {}
            config_data['concurrency']['max_workers'] = 1

            with open(config_yaml_path, 'w', encoding='utf-8') as f:
                yaml.dump(config_data, f, sort_keys=False)
            logger.success(f"âœ… '{config_yaml_path}' å·²æ›´æ–°ï¼Œè¨­å®š max_workers = 1ã€‚")

            # é©—è­‰ä¿®æ”¹
            # with open(config_yaml_path, 'r', encoding='utf-8') as f:
            #     logger.info(f"é©—è­‰ config.yaml å…§å®¹:\n{f.read()}")

        except Exception as e:
            logger.error(f"âŒ ä¿®æ”¹ '{config_yaml_path}' å¤±æ•—: {e}", exc_info=True)
    else:
        logger.error(f"âŒ config.yaml æ–‡ä»¶æœªåœ¨ '{config_yaml_path}' æ‰¾åˆ°ï¼Œç„¡æ³•ä¿®æ”¹ max_workersã€‚")

    logger.info(f"å·²å°‡å°ˆæ¡ˆè·¯å¾‘ '{OUTER_CLONE_PATH}' åŠ å…¥ç³»çµ±è·¯å¾‘ã€‚")
    logger.info(f"ç•¶å‰å·¥ä½œç›®éŒ„å·²åˆ‡æ›è‡³: {os.getcwd()}")
    logger.info(f"ç•¶å‰ sys.path: {sys.path}")

    # === éšæ®µä¸‰: å®‰è£ä¾è³´ ===
    logger.header("éšæ®µä¸‰: å®‰è£ä¾è³´")
    requirements_path = "requirements.txt" # ç¾åœ¨æ‡‰è©²åœ¨ OUTER_CLONE_PATH ä¸‹
    if not os.path.exists(requirements_path):
        logger.error(f"âŒ åœ¨ '{os.getcwd()}' ä¸­æ‰¾ä¸åˆ° {requirements_path}ã€‚ç„¡æ³•å®‰è£ä¾è³´ã€‚")
        hw_monitor.stop()
        return

    logger.info("æ­£åœ¨å®‰è£ä¾è³´...")
    pip_result = subprocess.run([sys.executable, "-m", "pip", "install", "-q", "-r", requirements_path], capture_output=True, text=True)
    if pip_result.returncode != 0:
        logger.error(f"âŒ Pip install å¤±æ•—ï¼\n{pip_result.stderr}", exc_info=False)
        hw_monitor.stop()
        return
    logger.success("âœ… å°ˆæ¡ˆä¾è³´å®‰è£æˆåŠŸï¼")

    # === éšæ®µå››: åŸ·è¡Œæ•¸æ“šç®¡é“ ===
    logger.header("éšæ®µå››: åŸ·è¡Œæ•¸æ“šç®¡é“ (æœ¬åœ°æ¨¡å¼)")
    commander = None
    try:
        # æ¸…ç†å¯èƒ½å·²ç·©å­˜çš„èˆŠæ¨¡çµ„
        modules_to_clear = ['data_pipeline.commander', 'data_pipeline', 'data_pipeline.fetchers.yfinance_fetcher']
        for module_name in modules_to_clear:
            if module_name in sys.modules:
                logger.info(f"å¾ sys.modules ä¸­ç§»é™¤å·²ç·©å­˜çš„æ¨¡çµ„: {module_name}")
                del sys.modules[module_name]

        from data_pipeline.commander import Commander # type: ignore
        logger.info("åˆå§‹åŒ–æŒ‡æ®å®˜...")

        db_full_path = os.path.join(LOCAL_DATA_PATH, "panoramic_analyzer.duckdb")
        cache_full_path = os.path.join(LOCAL_DATA_PATH, "api_cache.sqlite")

        logger.info(f"æ•¸æ“šåº«å°‡å‰µå»ºæ–¼æœ¬åœ°: {db_full_path}")
        logger.info(f"å¿«å–å°‡å‰µå»ºæ–¼æœ¬åœ°: {cache_full_path}")

        if os.path.exists(db_full_path): os.remove(db_full_path)
        if os.path.exists(cache_full_path): os.remove(cache_full_path)

        commander = Commander( # type: ignore
            config_path='config.yaml', # ç¾åœ¨æ‡‰è©²åœ¨ OUTER_CLONE_PATH ä¸‹
            db_path=db_full_path,
            cache_path=cache_full_path
        )

        logger.info(f"æŒ‡æ®å®˜ä¸‹é”æŒ‡ä»¤ï¼šåŸ·è¡Œæ‰¹æ¬¡æ•¸æ“šç²å–èˆ‡å„²å­˜ï¼Œç›®æ¨™: {TEST_SYMBOLS}")
        symbols_map_for_run = {'equity': TEST_SYMBOLS.split(',')}

        commander.run_batch_fetch_and_store( # type: ignore
            symbols_map=symbols_map_for_run,
            start_date=TEST_START_DATE,
            end_date=TEST_END_DATE
        )
        logger.success("âœ… æŒ‡æ®å®˜æ‰¹æ¬¡ä»»å‹™åŸ·è¡Œå®Œç•¢ï¼")

    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šç®¡é“åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
        hw_monitor.stop()
        return
    finally:
        if commander:
            commander.close() # type: ignore

    # === éšæ®µäº”: æ•¸æ“šé©—è­‰ ===
    logger.header("éšæ®µäº”: æ•¸æ“šåº«é©—è­‰ (æœ¬åœ°æ¨¡å¼)")
    db_full_path = os.path.join(LOCAL_DATA_PATH, "panoramic_analyzer.duckdb")
    if not os.path.exists(db_full_path):
        logger.error(f"âŒ é©—è­‰å¤±æ•—ï¼šé æœŸçš„æœ¬åœ°æ•¸æ“šåº«æª”æ¡ˆ {db_full_path} æœªæ‰¾åˆ°ï¼")
        hw_monitor.stop()
        return

    try:
        con = duckdb.connect(database=db_full_path, read_only=True)
        logger.info("âœ… æˆåŠŸé€£æ¥åˆ°æœ¬åœ° DuckDB æ•¸æ“šåº«é€²è¡Œé©—è­‰ã€‚")

        tables = con.execute("SHOW TABLES;").fetchdf()
        logger.info(f"æ•¸æ“šåº«ä¸­çš„è¡¨æ ¼:\n{tables}")

        table_to_check = "ohlcv_daily"
        if table_to_check in tables['name'].values: # type: ignore
            logger.success(f"âœ… è¡¨æ ¼ '{table_to_check}' å­˜åœ¨ã€‚")
            for symbol in TEST_SYMBOLS.split(','):
                symbol_upper = symbol.strip().upper() # ç¢ºå¯¶è‚¡ç¥¨ä»£ç¢¼æ˜¯å¤§å¯«ä¸”ç„¡ç©ºæ ¼
                count_result = con.execute(f"SELECT COUNT(*) FROM {table_to_check} WHERE symbol = ?", [symbol_upper]).fetchone()
                if count_result and count_result[0] > 0:
                    logger.success(f"  - æ‰¾åˆ° {symbol_upper}: {count_result[0]} ç­†è¨˜éŒ„ã€‚")
                else:
                    logger.warning(f"  - æœªåœ¨æ•¸æ“šåº«ä¸­æ‰¾åˆ° {symbol_upper} çš„è¨˜éŒ„ã€‚")
        else:
            logger.error(f"âŒ é©—è­‰å¤±æ•—ï¼šæ ¸å¿ƒè¡¨æ ¼ '{table_to_check}' ä¸å­˜åœ¨ï¼")

        con.close()
    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šåº«é©—è­‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

    # === æ”¶å°¾å·¥ä½œ ===
    hw_monitor.stop()
    duration = time.time() - start_time
    logger.header(f"ğŸ å…¨éƒ¨æµç¨‹åœ¨ {duration:.2f} ç§’å…§åŸ·è¡Œå®Œç•¢")

    log_filename = f"local_deployment_log_{datetime.now(TAIPEI_TZ).strftime('%Y%m%d_%H%M%S')}.log"
    # ä¿®æ”¹æ—¥èªŒä¿å­˜è·¯å¾‘åˆ° /tmp
    final_log_path = f"/tmp/{log_filename}"
    with open(final_log_path, "w", encoding="utf-8") as f:
        f.write(logger.get_full_log())
    logger.success(f"å®Œæ•´çš„ç´”æ–‡å­—æ—¥èªŒå·²ä¿å­˜è‡³: {final_log_path}")

    if IS_COLAB:
        print("\nè‹¥éœ€ä¸‹è¼‰æ—¥èªŒæª”æ¡ˆï¼Œè«‹åœ¨ä¸‹ä¸€å€‹å„²å­˜æ ¼åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š")
        print(f"from google.colab import files; files.download('{final_log_path}')")

if __name__ == '__main__':
    run_deployment_and_test()
