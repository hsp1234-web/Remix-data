# -*- coding: utf-8 -*-
# @title ğŸ“ˆ å…¨æ™¯å¸‚å ´åˆ†æå„€ v4.0 (ç”Ÿç”¢ç´šç©©å¥èˆ‡éŒ¯èª¤è™•ç†æœ€çµ‚ç‰ˆ)
# @markdown ### ç³»çµ±ä»‹ç´¹
# @markdown æœ¬è…³æœ¬æ˜¯ V6ã€Œå…¨æ™¯å¸‚å ´åˆ†æå„€ã€çš„ç”Ÿç”¢ç´šæ•¸æ“šé è™•ç†ç‰ˆæœ¬ã€‚å®ƒèåˆäº†å°ˆæ¥­ç´šçš„éŒ¯èª¤è™•ç†æ€æƒ³ï¼Œå¯¦ç¾äº†ä¸€å€‹æ¥µåº¦ç©©å¥ã€å…·å‚™ç‹€æ…‹è¨˜æ†¶çš„è‡ªå‹•åŒ–æ•¸æ“šè™•ç†ç®¡é“ã€‚
# @markdown - **éšæ¢¯å¼é™ç´šæŠ“å–**: å¯¦ç¾äº† `1h -> 1d -> 1wk` çš„æ™ºæ…§é™ç´šæŠ“å–ç­–ç•¥ï¼Œç¢ºä¿åœ¨ä»»ä½•æƒ…æ³ä¸‹éƒ½èƒ½ç²å–æœ€ç²¾ç´°çš„å¯ç”¨æ•¸æ“šã€‚
# @markdown - **ç”Ÿç”¢ç´šéŒ¯èª¤è™•ç† (æ ¸å¿ƒå‡ç´š)**: æ ¹æ“šæ‚¨æä¾›çš„å°ˆæ¥­ç¯„æœ¬ï¼Œå¾¹åº•è§£æ±ºäº†æ‰€æœ‰å·²çŸ¥çš„ `ValueError` å’Œ `TypeError`ï¼Œä¸¦ç‚ºæ‰€æœ‰ API å‘¼å«å¢åŠ äº†ç¨ç«‹ã€ç´°ç·»çš„ `try-except` ä¿è­·ï¼Œç¢ºä¿ä»»ä½•å–®ä¸€çš„æ•¸æ“šæŠ“å–å¤±æ•—éƒ½ä¸æœƒä¸­æ–·æ•´é«”æµç¨‹ã€‚
# @markdown - **çœŸå¯¦æ•¸æ“šæ•´åˆèˆ‡å¢é‡è™•ç†**: æ­£å¼å‘¼å« `yfinance` APIï¼Œä¸¦é€é SHA-256 é›œæ¹Šå€¼åµæ¸¬å¯¦ç¾äº†é«˜æ•ˆçš„å¢é‡è™•ç†ã€‚
# @markdown - **ä¼æ¥­ç´šæŠ“å–ç©©å®šæ€§**: å…§å»ºå»¶é²ã€æŠ–å‹•ã€æŒ‡æ•¸é€€é¿é‡è©¦åŠæœ¬åœ°å¿«å–åŠŸèƒ½ã€‚
# @markdown ---
# @markdown ### å°ˆæ¡ˆç‰ˆæœ¬æ›´æ–°æ—¥èªŒ (Changelog)
# @markdown - **`v4.0` (ç•¶å‰ç‰ˆæœ¬) - ç”Ÿç”¢ç´šç©©å¥èˆ‡éŒ¯èª¤è™•ç†**
# @markdown   - **æ ¸å¿ƒé‡æ§‹**: æ ¹æ“šæ‚¨æä¾›çš„ `Cell 5` éŒ¯èª¤è™•ç†ç¯„æœ¬ï¼Œé‡æ§‹äº† `generate_deep_dive_text` å‡½å¼ï¼Œç‚ºå…¶ä¸­çš„æ¯ä¸€å€‹ API å‘¼å«éƒ½å¢åŠ äº†ç¨ç«‹çš„ `try-except` ä¿è­·ï¼Œä¸¦å°æ‰€æœ‰ Pandas DataFrame é€²è¡Œäº†åš´æ ¼çš„ç©ºå€¼èˆ‡æ¬„ä½æª¢æŸ¥ã€‚
# @markdown   - **éŒ¯èª¤ä¿®æ­£**: å¾¹åº•è§£æ±ºäº†å› ä¸ç•¶çš„ Pandas DataFrame åˆ¤æ–·æ‰€å°è‡´çš„ `ValueError` å’Œ `TypeError`ã€‚
# @markdown - **`v3.0`-`v3.1` - éšæ¢¯å¼é™ç´šå¼•æ“èˆ‡åˆæ­¥ä¿®æ­£**
# @markdown   - å¯¦ä½œäº†ã€Œéšæ¢¯å¼é™ç´šã€æ•¸æ“šæŠ“å–ç­–ç•¥ï¼Œä¸¦å° `v2.x` çš„éŒ¯èª¤é€²è¡Œäº†åˆæ­¥ä¿®æ­£ã€‚
# @markdown - **`v2.0`-`v2.2` - Google Drive æ•´åˆèˆ‡éŒ¯èª¤ç™¼ç¾éšæ®µ**
# @markdown   - å¼•å…¥ `IN/OUT` è³‡æ–™å¤¾åŠé›œæ¹Šå€¼å¢é‡è™•ç†ã€‚åœ¨æ­¤éç¨‹ä¸­ç™¼ç¾äº† `NameError`, `TypeError`, `ValueError` ç­‰å¯¦æˆ°å•é¡Œã€‚
# @markdown - **`v1.0` - å…¨æ™¯å¸‚å ´åˆ†æå„€ (æœ¬åœ°æ²™ç›¤æ¨æ¼”)**
# @markdown   - é€éå…§å»ºæ¨¡æ“¬å™¨å®Œæ•´å¯¦ç¾ä¸¦é©—è­‰äº† V6 æ–¹æ¡ˆçš„æ ¸å¿ƒé‚è¼¯ã€‚

# ==============================================================================
# æ­¥é©Ÿ 0: ç’°å¢ƒè¨­å®šèˆ‡å‡½å¼åº«å°å…¥
# ==============================================================================
!pip install -q yfinance

import os
import re
import time
import json
import hashlib
import random
from datetime import datetime, timedelta
import pandas as pd
import yfinance as yf
from google.colab import drive

try:
    drive.mount('/content/drive')
    print("Google Drive æ›è¼‰æˆåŠŸã€‚")
except Exception as e:
    print(f"Google Drive æ›è¼‰å¤±æ•—: {e}")

# ==============================================================================
# æ­¥é©Ÿ 1: å…¨å±€è¨­å®šå€
# ==============================================================================
BASE_PROJECT_PATH = '/content/drive/MyDrive/MyWeeklyAnalysisProject'
IN_FOLDER_PATH = os.path.join(BASE_PROJECT_PATH, 'IN_Source_Reports')
OUT_FOLDER_PATH = os.path.join(BASE_PROJECT_PATH, 'OUT_Processed_Prompts')
CACHE_FOLDER_PATH = os.path.join(BASE_PROJECT_PATH, 'CACHE_Market_Data')
MANIFEST_FILE_PATH = os.path.join(OUT_FOLDER_PATH, 'processed_files_manifest.json')

MACRO_TICKERS = [
    '^TWII', '^GSPC', '^IXIC', '^DJI',
    'TLT', '^IRX', '^FVX', '^TNX', '^TYX',
    '^VIX', '^MOVE',
    'CL=F', 'GC=F'
]
BELLWETHER_STOCKS = {
    'AAPL': 'è˜‹æœ', 'MSFT': 'å¾®è»Ÿ', 'NVDA': 'è¼é”', 'TSLA': 'ç‰¹æ–¯æ‹‰',
    '2330.TW': 'å°ç©é›»', '2454.TW': 'è¯ç™¼ç§‘', 'JPM': 'æ‘©æ ¹å¤§é€š', 'WMT': 'æ²ƒçˆ¾ç‘ª'
}

# ==============================================================================
# æ­¥é©Ÿ 2: æ ¸å¿ƒåŠŸèƒ½å‡½å¼åº«
# ==============================================================================

def get_file_hash(filepath):
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def load_manifest():
    if os.path.exists(MANIFEST_FILE_PATH):
        with open(MANIFEST_FILE_PATH, 'r', encoding='utf-8') as f:
            try: return json.load(f)
            except json.JSONDecodeError: return {}
    return {}

def save_manifest(manifest):
    with open(MANIFEST_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=4)

def robust_yfinance_fetch(ticker, start, end, interval_priority=['1h', '1d', '1wk'], retries=3, backoff_factor=1):
    for interval in interval_priority:
        cache_filename = f"{ticker}_{start}_{end}_{interval}.pkl".replace(':', '-')
        cache_filepath = os.path.join(CACHE_FOLDER_PATH, cache_filename)
        if os.path.exists(cache_filepath):
            print(f"  - (å¿«å–å‘½ä¸­) æ­£åœ¨å¾æœ¬åœ°è¼‰å…¥ {ticker} ({interval}) æ•¸æ“š...")
            return pd.read_pickle(cache_filepath), interval

        print(f"  - (ç¶²è·¯æŠ“å–) æ­£åœ¨å˜—è©¦ {ticker} ({interval})...")
        for i in range(retries):
            try:
                time.sleep(random.uniform(0.5, 1.5))
                data = yf.download(ticker, start=start, end=end, interval=interval, progress=False, auto_adjust=True)

                if data is not None and not data.empty:
                    print(f"  - âœ… æˆåŠŸæŠ“å–ä¸¦å¿«å– {ticker} ({interval})ã€‚")
                    data.to_pickle(cache_filepath)
                    return data, interval
                else:
                    print(f"  - {ticker} ({interval}) ç„¡æ•¸æ“šï¼Œå˜—è©¦ä¸‹ä¸€å€‹é€±æœŸ...")
                    break
            except Exception as e:
                wait_time = backoff_factor * (2 ** i)
                print(f"  - æŠ“å– {ticker} ({interval}) å¤±æ•— (ç¬¬ {i+1}/{retries} æ¬¡): {str(e)[:150]}...")
                if i < retries - 1:
                    time.sleep(wait_time)
                else:
                    print(f"  - å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œå˜—è©¦ä¸‹ä¸€å€‹é€±æœŸ...")

    print(f"  - âŒ æ”¾æ£„æŠ“å– {ticker}ï¼Œæ‰€æœ‰é€±æœŸå‡å˜—è©¦å¤±æ•—ã€‚")
    failed_cache_path = os.path.join(CACHE_FOLDER_PATH, f"{ticker}_{start}_{end}_FAILED.pkl".replace(':', '-'))
    pd.DataFrame().to_pickle(failed_cache_path)
    return pd.DataFrame(), 'N/A'

def get_candlestick_feature(row):
    if not all(k in row and pd.notna(row[k]) for k in ['Open', 'Close', 'High', 'Low']): return "æ•¸æ“šä¸å…¨"
    if row['High'] == row['Low']: return "ç„¡æ³¢å‹•"
    body_size = abs(row['Open'] - row['Close'])
    full_range = row['High'] - row['Low']
    if full_range == 0 or body_size / full_range < 0.2: return "åå­—æ˜Ÿ"
    return "é•·ç´…Kç·š" if row['Close'] > row['Open'] else "é•·é»‘Kç·š"

def generate_focus_context_text(ticker, total_start, total_end, focus_start, focus_end):
    full_daily_data, daily_interval = robust_yfinance_fetch(ticker, total_start, total_end, interval_priority=['1d', '1wk'])
    if full_daily_data.empty: return f"# {ticker}\n- ç„¡æ³•ç²å–åŸºç¤æ—¥ç·š/é€±ç·šæ•¸æ“šã€‚\n"

    full_hourly_data, hourly_interval = robust_yfinance_fetch(ticker, total_start, total_end, interval_priority=['1h'])

    output_lines = [f"# {ticker} - å®è§€å¸‚å ´æ•¸æ“š (ä¸»è¦é€±æœŸ: {daily_interval})\n"]
    output_lines.append("# PART 1: å®è§€ä¸Šä¸‹æ–‡ (å‰å¾Œå„2å€‹æœˆ) - æ¯æ—¥é‡åŒ–æ—¥èªŒ")

    avg_volume = full_daily_data['Volume'].mean() if 'Volume' in full_daily_data and not full_daily_data['Volume'].empty else 1
    context_data = full_daily_data[(full_daily_data.index.date < focus_start.date()) | (full_daily_data.index.date > focus_end.date())]

    for date, row in context_data.iterrows():
        daily_change = (row['Close'] / row['Open'] - 1) * 100 if row.get('Open', 0) > 0 else 0
        k_feature = get_candlestick_feature(row)
        volume_rating = row.get('Volume', 0) / avg_volume if avg_volume > 0 else 0
        log_line = f"- {date.strftime('%Y-%m-%d')}: {daily_change:+.1f}%, {k_feature}, æˆäº¤é‡:{volume_rating:.1f}x"
        output_lines.append(log_line)

        if volume_rating > 3.0 and not full_hourly_data.empty:
            output_lines.append("  - [åµæ¸¬åˆ°é—œéµäº‹ä»¶æ—¥ï¼šé™„ä¸Šç•¶æ—¥å®Œæ•´å°æ™‚ç·šæ•¸æ“š]")
            anomaly_hourly_data = full_hourly_data[full_hourly_data.index.date == date.date()]
            if not anomaly_hourly_data.empty:
                output_lines.append("    " + anomaly_hourly_data.to_string().replace('\n', '\n    '))

    output_lines.append(f"\n# PART 2: ç„¦é»åˆ†æå€ (æ ¸å¿ƒé€±: {focus_start.strftime('%Y-%m-%d')} to {focus_end.strftime('%Y-%m-%d')})")

    focus_data, focus_interval = (full_hourly_data, hourly_interval) if not full_hourly_data.empty else (full_daily_data, daily_interval)
    focus_data_in_range = focus_data[(focus_data.index.date >= focus_start.date()) & (focus_data.index.date <= focus_end.date())]

    if not focus_data_in_range.empty:
        output_lines.append(f"- æä¾›æœ¬é€±æ•¸æ“š (æ•¸æ“šé€±æœŸ: {focus_interval})")
        output_lines.append(focus_data_in_range.to_string())
    else:
        output_lines.append("- è­¦å‘Š: æ ¸å¿ƒé€±ç„¡å¯ç”¨æ•¸æ“šã€‚")

    return "\n".join(output_lines)

def generate_deep_dive_text(ticker):
    print(f"  - æ­£åœ¨å‰–æé¾é ­ä¼æ¥­: {ticker}...")
    lines = []
    try:
        stock = yf.Ticker(ticker)
        time.sleep(random.uniform(0.2, 0.5))

        try:
            info = stock.info
            lines.append(f"- **{info.get('shortName', ticker)} ({ticker})**")
            market_cap = info.get('marketCap')
            market_cap_str = f"{market_cap / 1e9:.1f}B" if market_cap else "N/A"
            lines.append(f"  - åŸºæœ¬é¢: ç”¢æ¥­({info.get('industry', 'N/A')}), å¸‚å€¼({market_cap_str})")
        except Exception as e:
            lines.append(f"- **{BELLWETHER_STOCKS.get(ticker, ticker)} ({ticker})**")
            lines.append(f"  - è­¦å‘Š: ç„¡æ³•ç²å–åŸºæœ¬è³‡è¨Š. Error: {e}")
            # å³ä½¿ info å¤±æ•—ï¼Œä¹Ÿç¹¼çºŒå˜—è©¦å…¶ä»–æ•¸æ“š
            pass

        try:
            financials = stock.quarterly_financials
            if financials is not None and not financials.empty:
                financials = financials.T
                if 'Total Revenue' in financials.columns and len(financials) >= 5 and financials['Total Revenue'].iloc[4] is not None and financials['Total Revenue'].iloc[4] > 0:
                    rev_g = (financials['Total Revenue'].iloc[0] / financials['Total Revenue'].iloc[4] - 1) * 100
                    if lines: lines[-1] += f", æœ€æ–°ç‡Ÿæ”¶å¹´å¢({rev_g:.1f}%)"
        except Exception as e:
            print(f"    - è­¦å‘Š: ç„¡æ³•ç²å– {ticker} çš„è²¡å‹™æ•¸æ“š. Error: {e}")
            lines.append("  - è²¡å‹™æ•¸æ“š: N/A")

        try:
            time.sleep(random.uniform(0.2, 0.5))
            recom = stock.recommendations
            if recom is not None and not recom.empty:
                lines.append(f"  - åˆ†æå¸«æƒ…ç·’: {recom['To Grade'].tail(5).value_counts().to_dict()}")
        except Exception as e:
            print(f"    - è­¦å‘Š: ç„¡æ³•ç²å– {ticker} çš„åˆ†æå¸«è©•ç´š. Error: {e}")
            lines.append("  - åˆ†æå¸«æƒ…ç·’: N/A")

        try:
            time.sleep(random.uniform(0.2, 0.5))
            news = stock.news
            if news and isinstance(news, list) and len(news) > 0:
                lines.append(f"  - é—œéµæ–°è: \"{news[0]['title']}\" ({news[0]['publisher']})")
        except Exception as e:
            print(f"    - è­¦å‘Š: ç„¡æ³•ç²å– {ticker} çš„æ–°è. Error: {e}")
            lines.append("  - é—œéµæ–°è: N/A")

        return "\n".join(lines)

    except Exception as e:
        print(f"  - å‰–æ {ticker} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        return f"- **{BELLWETHER_STOCKS.get(ticker, ticker)} ({ticker})**\n  - ç„¡æ³•ç²å–è©³ç´°æ•¸æ“šã€‚"

# ==============================================================================
# æ­¥é©Ÿ 4: ä¸»åŸ·è¡Œå‡½å¼
# ==============================================================================
def run_panoramic_processor():
    print("="*80)
    print("ğŸš€ å•Ÿå‹• V6 å…¨æ™¯å¸‚å ´åˆ†æå„€ (v4.0 - ç”Ÿç”¢ç´šç©©å¥ç‰ˆ)")
    print("="*80)

    os.makedirs(IN_FOLDER_PATH, exist_ok=True)
    os.makedirs(OUT_FOLDER_PATH, exist_ok=True)
    os.makedirs(CACHE_FOLDER_PATH, exist_ok=True)
    print(f"è¼¸å…¥è³‡æ–™å¤¾ (IN): {IN_FOLDER_PATH}")
    print(f"è¼¸å‡ºè³‡æ–™å¤¾ (OUT): {OUT_FOLDER_PATH}")
    print(f"å¿«å–è³‡æ–™å¤¾ (CACHE): {CACHE_FOLDER_PATH}")

    manifest = load_manifest()
    print(f"\n[ç‹€æ…‹] ç™¼ç¾ {len(manifest)} å€‹å·²è™•ç†éçš„æª”æ¡ˆç´€éŒ„ã€‚")

    try:
        source_files = [f for f in os.listdir(IN_FOLDER_PATH) if f.endswith('.txt') and 'Geminiè™•ç†' in f]
        print(f"[æƒæ] åœ¨è¼¸å…¥è³‡æ–™å¤¾ä¸­æ‰¾åˆ° {len(source_files)} å€‹ç›®æ¨™ .txt æª”æ¡ˆã€‚")
    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¼¸å…¥è³‡æ–™å¤¾ '{IN_FOLDER_PATH}'ã€‚")
        return

    new_files_to_process = [f for f in source_files if manifest.get(f) != get_file_hash(os.path.join(IN_FOLDER_PATH, f))]

    if not new_files_to_process:
        print("\n[å®Œæˆ] æ²’æœ‰æ–°çš„æˆ–ä¿®æ”¹éçš„æª”æ¡ˆéœ€è¦è™•ç†ã€‚")
        return

    print(f"\n[ä»»å‹™] æº–å‚™è™•ç† {len(new_files_to_process)} å€‹æ–°æª”æ¡ˆ...")
    print("-" * 80)

    for filename in new_files_to_process:
        print(f"\nğŸ”¥ é–‹å§‹è™•ç†æª”æ¡ˆ: {filename}")
        filepath = os.path.join(IN_FOLDER_PATH, filename)

        try:
            match = re.search(r'(\d{4})å¹´ç¬¬(\d{1,2})é€±', filename)
            if not match:
                print(f"  - éŒ¯èª¤ï¼šç„¡æ³•å¾ '{filename}' è§£ææ—¥æœŸï¼Œå·²è·³éã€‚")
                continue

            year, week_num = map(int, match.groups())
            focus_start = datetime.fromisocalendar(year, week_num, 1)
            focus_end = focus_start + timedelta(days=6)
            total_start_dt = focus_start - timedelta(days=60)
            total_end_dt = focus_end + timedelta(days=60)
            total_start, total_end = total_start_dt.strftime('%Y-%m-%d'), total_end_dt.strftime('%Y-%m-%d')
            print(f"  - [1/4] å·²è¨ˆç®—æ—¥æœŸ -> ç„¦é»é€±: {focus_start.strftime('%Y-%m-%d')} to {focus_end.strftime('%Y-%m-%d')}")

            print(f"\n  - [2/4] æ­£åœ¨æŠ“å–å®è§€å¸‚å ´æ•¸æ“š...")
            macro_context_text = []
            for ticker in MACRO_TICKERS:
                text = generate_focus_context_text(ticker, total_start, total_end, focus_start, focus_end)
                macro_context_text.append(text)

            print(f"\n  - [3/4] æ­£åœ¨å‰–æé¾é ­ä¼æ¥­åŸºæœ¬é¢...")
            deep_dive_texts = []
            for ticker in BELLWETHER_STOCKS.keys():
                text = generate_deep_dive_text(ticker)
                deep_dive_texts.append(text)

            with open(filepath, 'r', encoding='utf-8') as f:
                report_content = f.read()

            final_prompt = f"""# AI ç­–ç•¥åˆ†æä»»å‹™ (é è™•ç†æ–‡æœ¬)
# é€±å ±æ¨™é¡Œ: {filename}
---
## è³ªåŒ–é€±å ±å…§æ–‡
{report_content}
---
## PART 1: å®è§€å¸‚å ´ä¸Šä¸‹æ–‡
{chr(10).join(macro_context_text)}
---
## PART 2: é¾é ­ä¼æ¥­åŸºæœ¬é¢å‰–æ
{chr(10).join(deep_dive_texts)}
---
## ä½ çš„ä»»å‹™èˆ‡ç”¢å‡ºè¦æ±‚
[æ­¤è™•å°‡æ˜¯æ‚¨æä¾›çµ¦ Gemini çš„æœ€çµ‚æŒ‡ä»¤]
"""
            print(f"\n  - [4/4] å·²ç”Ÿæˆæœ€çµ‚åˆ†æç”¨ Promptã€‚")

            output_filename = f"PROMPT_{os.path.splitext(filename)[0]}.txt"
            output_filepath = os.path.join(OUT_FOLDER_PATH, output_filename)
            with open(output_filepath, 'w', encoding='utf-8') as f:
                f.write(final_prompt)

            manifest[filename] = get_file_hash(filepath)
            save_manifest(manifest)

            print(f"  - âœ… æˆåŠŸï¼å·²å°‡è™•ç†çµæœå„²å­˜è‡³: {output_filepath}")

        except Exception as e:
            print(f"è™•ç†æª”æ¡ˆ '{filename}' æ™‚ç™¼ç”Ÿä¸å¯é æœŸçš„åš´é‡éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            print(f"å°‡è·³éæ­¤æª”æ¡ˆï¼Œç¹¼çºŒä¸‹ä¸€å€‹ã€‚")
            continue

    print("\n\n" + "="*80)
    print("ğŸ‰ æ‰€æœ‰æ–°æª”æ¡ˆè™•ç†å®Œç•¢ï¼")
    print("="*80)

# --- åŸ·è¡Œä¸»ç¨‹å¼ ---
run_panoramic_processor()
