# 專案開發計畫：全自動本地智慧數據管道

## 專案目標

建立一套完整、全自動、且高效的本地智慧數據解決方案，用於處理龐大且多樣的期交所數據。系統需具備高適應性、高穩定性與高效能，能夠自動偵測檔案格式、進行數據清洗與轉換，並將結果存儲於結構化資料庫中，同時提供完善的日誌與監控機制。

## 一、格式指紋目錄 (Format Fingerprint Catalog)

此組件用於實現檔案格式的自動偵測與處理分派。

### 1.1 指紋定義方式

*   **策略**：「多行嗅探，首個匹配」。
    *   讀取檔案的前 N 行（例如 20 行）。
    *   從上到下逐行檢查，第一個符合「標頭特徵」的行被視為標頭行。
        *   標頭特徵判斷標準：例如，以逗號分隔的欄位數量最多，且/或包含某些預期一定會出現的關鍵字（如「交易日期」、「契約」等）。
*   **正規化流程**：
    1.  提取標頭行的所有欄位名稱。
    2.  對每個欄位名稱：
        *   **完全清除空白**：清除欄位名稱內部所有空白字元及首尾空白（例如 ` " 欄位 名稱 " ` -> `"欄位名稱"`）。
        *   **轉為小寫**：將欄位名稱轉換為全小寫。
    3.  將處理後的欄位名依照**字母順序**排序。
    4.  使用**固定的單一分隔符 `|`** （管道符）將排序後的欄位名合併成一個字串（例如：`"欄位a|欄位b|欄位c"`）。
    5.  計算該合併字串的 **SHA256 雜湊值**，此雜湊值即為該檔案格式的唯一「指紋」。
*   **ZIP 檔案處理**：指紋是針對解壓縮後的**個別檔案**計算。

### 1.2 處理配方 (Recipe) 資訊

每個「指紋」在「格式指紋目錄」中會對應一個「處理配方」，包含以下關鍵資訊：

1.  `target_table` (字串)：數據應存入的最終目標資料庫表格名稱 (例如 `fact_daily_ohlc`)。
2.  `parser_config` (物件/字典)：傳遞給 `pandas` 讀取函式（如 `read_csv`）的特定參數。這應設計為可以直接解包 (unpack) 的字典，以提供最大靈活性。
    *   範例：`{"sep": ",", "skiprows": 1, "encoding": "ms950", "header": null, "dtype": {"成交價格": "float"}}`
3.  `cleaner_function` (字串)：對應的專用數據清洗函式名稱 (例如 `clean_daily_ohlc_v1`)。
4.  `required_columns` (陣列)：一個包含該格式所有必要欄位名稱的列表。用於在解析後驗證 DataFrame 是否包含所有預期欄位，確保數據完整性。
5.  `description` (字串, 可選)：對此格式的人類可讀描述，例如「期交所每日個股期貨OHLCV (格式版本1)」。

### 1.3 `format_catalog.json` 檔案結構範例

```json
{
  "a1b2c3d4e5f67890...": { // <-- 正規化標頭的 SHA256 指紋
    "description": "期交所每日個股期貨OHLCV (特定格式版本)",
    "target_table": "fact_daily_ohlc",
    "parser_config": {
      "sep": ",",
      "skiprows": 1,
      "encoding": "ms950"
      // ... 其他 pandas 讀取參數 ...
    },
    "cleaner_function": "clean_futures_ohlcv_type1",
    "required_columns": ["交易日期", "契約代號", "開盤價", "最高價", "最低價", "收盤價", "成交量"]
  },
  "another_fingerprint_hash...": {
    // ... 另一個檔案格式的配方 ...
  }
}
```

### 1.4 清洗函式 (`cleaner_function`) 介面

*   **標準化函式簽名 (Standard Signature)**：所有清洗函式都應遵循以下介面：
    *   輸入：接收一個 `pandas.DataFrame` 物件。
    *   返回：返回一個**新的**、已清洗乾淨的 `pandas.DataFrame` 物件。
    *   範例：`def clean_specific_format(df: pd.DataFrame) -> pd.DataFrame:`

## 二、兩階段自動化管線 (Two-Stage Automated Pipeline)

將數據處理明確切分為「汲取」和「轉換」兩個獨立階段，以實現解耦合、穩定性與效率。

### 2.1 第一階段：汲取管線 (Ingestion Pipeline)

*   **核心原則**：「**極速、穩定、零解析**」。唯一目標是將原始數據安全、完整、快速地收入 `raw_lake.db`。
*   **作業流程**：
    1.  **掃描來源**：監控指定的資料夾（如 Google Drive 上的 `MyTaifexDataProject/00_input_files/` 或 API/爬蟲數據的 `MyTaifexDataProject/data/00_landing_zone/`）。
    2.  **比對清單 (Manifest Check)**：對於掃描到的每個檔案：
        *   計算其**內容的 SHA256 雜湊值**。
        *   查詢 `manifest.db`，若該雜湊值已存在且狀態為 `RAW_INGESTED` 或 `TRANSFORMATION_SUCCESS`，則**自動跳過**，避免重複處理。
    3.  **原始入庫 (Raw Ingestion)**：對於新檔案（或 `manifest.db` 中未成功汲取的檔案）：
        *   直接將其**未經修改的原始二進位內容**存入 `raw_lake.db`。
    4.  **登記狀態 (Manifest Update)**：在 `manifest.db` 中記錄或更新該檔案的資訊（詳見 2.4 `manifest.db` 設計），並將狀態更新為 `RAW_INGESTED`。
*   **`raw_lake.db` 設計**：
    *   建議使用 SQLite 或 DuckDB。
    *   資料表結構（例如 `raw_files`）：
        *   `file_hash (TEXT, PRIMARY KEY)`：檔案內容的 SHA256 雜湊值。
        *   `raw_content (BLOB)`：檔案的原始二進位內容。
    *   **理由**：BLOB 存入資料庫確保數據完整性與原子性，管理簡潔，效能可接受。

### 2.2 第二階段：轉換管線 (Transformation Pipeline)

*   **核心原則**：「**智慧、平行、可重跑**」。負責所有重量級運算，必須能被獨立、安全地重複執行。
*   **作業流程**：
    1.  **讀取任務**：啟動時，查詢 `manifest.db`，獲取所有狀態為 `RAW_INGESTED` (或特定重跑狀態，如 `QUARANTINED` 且使用者已更新配方) 的檔案清單。
    2.  **平行分派**：使用 `ProcessPoolExecutor`，將任務清單中的檔案平均分配給所有可用的 CPU 核心（詳見 3.1 CPU核心利用），啟動多個「工人 (Worker)」程序同時作業。
    3.  **單一工人任務 (Single Worker Task)**：每個工人對分配到的檔案執行以下操作：
        *   從 `raw_lake.db` 根據 `file_hash` 讀取其原始二進位內容。
        *   將原始內容轉換為 `io.BytesIO` 串流。
        *   計算其**格式指紋**（依據 1.1 指紋定義方式）。
        *   查詢 `format_catalog.json`（格式指紋目錄）獲取對應的**處理配方**。
        *   **若找到配方**：
            *   使用配方中的 `parser_config` 和原始串流，調用相應的 `pandas` 讀取函式將數據解析為 DataFrame。
            *   驗證 DataFrame 是否包含配方中 `required_columns` 指定的所有欄位。若不符合，視為處理失敗。
            *   調用配方中指定的 `cleaner_function` 對 DataFrame 進行清洗與轉換。
            *   將清洗後的乾淨 DataFrame 載入最終目標資料庫中配方指定的 `target_table`。
            *   返回處理成功狀態及相關統計（如處理行數）。
        *   **若找不到配方**（新格式）：
            *   放棄處理，返回「待隔離 (Quarantined)」狀態。
        *   **若過程中發生任何錯誤**（解析失敗、清洗失敗、驗證失敗、載入失敗）：
            *   記錄詳細錯誤訊息，返回「處理失敗 (Transformation Failed)」狀態。
    4.  **更新狀態 (Manifest Update)**：主流程等待所有工人完成後，根據每個任務的返回結果，將 `manifest.db` 中對應檔案的狀態更新為 `TRANSFORMATION_SUCCESS`、`QUARANTINED` 或 `TRANSFORMATION_FAILED`，並記錄相關元數據（如 `transformation_timestamp`, `fingerprint_hash`, `target_table_name`, `processed_row_count`, `error_message`）。

### 2.3 解耦合 (Decoupling) 的優勢

*   汲取階段的任何問題（如下載中斷、API來源暫時失效）不會影響已入庫數據的轉換。
*   轉換階段的任何錯誤（如遇到一個真正無法處理的髒數據、清洗邏輯Bug）也絕不會污染原始的 `raw_lake.db`。可以隨時修復清洗邏輯或更新 `format_catalog.json`，然後只針對失敗或隔離的檔案安全地「重跑」一次轉換管線。

### 2.4 `manifest.db` 設計 (審計與監控日誌)

`manifest.db` 不僅是狀態記錄，更是完整的審計與監控日誌。

*   建議使用 SQLite 或 DuckDB。
*   資料表結構（例如 `file_processing_log`）：
    *   `file_hash (TEXT, PRIMARY KEY)`：檔案內容的 SHA256 雜湊值。
    *   `original_file_path (TEXT)`：檔案在來源系統的原始路徑或識別碼，用於追溯。
    *   `status (TEXT)`：檔案目前的處理狀態，例如 `RAW_INGESTED`, `TRANSFORMATION_SUCCESS`, `TRANSFORMATION_FAILED`, `QUARANTINED`。
    *   `fingerprint_hash (TEXT, NULLABLE)`：成功識別格式後，對應到 `format_catalog.json` 的格式指紋。
    *   `ingestion_timestamp (DATETIME)`：檔案成功汲取到 `raw_lake.db` 的時間戳。
    *   `transformation_timestamp (DATETIME, NULLABLE)`：檔案成功轉換（或嘗試轉換失敗）的時間戳。
    *   `target_table_name (TEXT, NULLABLE)`：若轉換成功，數據存入的目標資料庫表名。
    *   `processed_row_count (INTEGER, NULLABLE)`：若轉換成功，從該檔案轉換出的數據行數。
    *   `error_message (TEXT, NULLABLE)`：若處理過程中發生錯誤，記錄詳細的錯誤訊息。
    *   `pipeline_execution_id (TEXT, NULLABLE)`：執行該處理的管線運行ID，便於追蹤單次運行的所有操作。

### 2.5 錯誤處理與隔離區 (`QUARANTINED`) 管理

*   **通知機制**：管線在每次執行結束時，透過日誌總結（詳見 4. 日誌輸出）明確報告：「本次運行發現 X 個新格式檔案（狀態為 `QUARANTINED`），已將其隔離。請手動進行格式註冊。」
*   **介入與重處理機制**：
    1.  **介入 (Manual Intervention)**：使用者根據日誌通知，找到被隔離的檔案。可以開發一個「格式註冊」輔助腳本，協助使用者為這些檔案生成指紋，並在 `format_catalog.json` 中手動添加或更新處理配方。
    2.  **重處理 (Reprocessing)**：為轉換管線的主啟動腳本（例如 `run.py`）設計一個專門的運行模式，例如增加一個 `--reprocess-quarantined` 的命令行參數。當使用此參數運行時，轉換管線的任務來源將查詢 `manifest.db` 中 `status = 'QUARANTINED'` 的檔案，並對它們重新執行一次完整的轉換流程。成功後狀態更新為 `TRANSFORMATION_SUCCESS`，失敗則保持 `QUARANTINED` 並更新錯誤訊息。

### 2.6 API 或爬蟲數據的汲取

*   **標準化流程**：為每種外部數據源（如 yfinance, FinMind API, 特定網站爬蟲）建立一個獨立的「**採集器 (Collector)**」模組。
*   **採集器職責**：
    1.  負責與其對應的外部 API 或網站進行通訊，獲取數據。
    2.  將獲取到的原始數據（例如，API 回應的 JSON 字串、爬取的 HTML 內容）直接保存成一個檔案（可加上時間戳命名），並放入一個專門的「**落地資料夾 (Landing Zone)**」（例如 `MyTaifexDataProject/data/00_landing_zone/`)。
    3.  主「汲取管線」會像監控其他來源資料夾一樣，定期掃描這個「落地資料夾」，將這些新生成的「原始檔案」納入後續標準的汲取與轉換流程中。
*   **優勢**：實現關注點分離。採集器專注於獲取數據，核心處理管線面對的永遠是「檔案」，簡化系統設計並易於擴展新的數據源。

## 三、資源最大化與狀態管理 (Resource Maximization & State Management)

（狀態管理主要已在 `manifest.db` 中詳細闡述）

### 3.1 CPU 核心利用 (Parallel Processing)

*   **策略**：「**動態偵測，全核利用**」。
*   **實現方式**：在「轉換管線 (Transformation Pipeline)」啟動時，程式應自動呼叫 `os.cpu_count()` 來獲取當前執行環境（如 Colab）分配到的 CPU 核心數。然後，將 `ProcessPoolExecutor` 的 `max_workers` 參數直接設為這個偵測到的核心數，以最大化平行處理能力。

### 3.2 記憶體管理 (Memory Management)

*   **策略**：「**安全預算**」，防止因單一大型檔案或資料庫操作導致記憶體溢位。
*   **實現方式**：
    1.  **資料庫記憶體限制**：若使用 DuckDB，在連接資料庫時，以程式化方式設定其記憶體使用上限，例如 `SET memory_limit = '70%'` （系統總記憶體的70%）。
    2.  **大檔案分塊處理 (Chunking)**：對於可能出現的超大檔案（例如 >1GB），在 `format_catalog.json` 中對應的 `parser_config` 可以包含 `chunksize` 參數。相應的 `pandas` 讀取函式和後續的 `cleaner_function` 應設計為能夠以「流式 (Streaming)」方式分塊處理數據，而非一次性將整個檔案載入記憶體。

## 四、日誌輸出 (Logging Output)

採用「**雙軌制日誌系統**」，兼顧即時操作反饋和後續深度分析。

### 4.1 軌道一：主控台即時報告 (Console Output)

*   **目標對象**：操作者（使用者）。
*   **形式**：在主控台（如 Colab Notebook輸出區域）輸出**人類易讀的**、簡潔的狀態更新。可考慮使用不同顏色或圖標表示不同級別的訊息。
    *   範例：`[INFO] 汲取階段啟動...`、`[SUCCESS] 檔案 XYZ.csv (Hash: abc...) 已成功轉換並載入到 fact_daily_ohlc。`、`[ERROR] 檔案 ABC.zip (Hash: def...) 處理失敗: 找不到格式配方。`
*   **目的**：提供即時、直觀的進度反饋和關鍵事件通知。

### 4.2 軌道二：結構化日誌檔案 (Structured Log File)

*   **目標對象**：開發者、機器分析、長期歸檔。
*   **形式**：將每一條日誌事件，以 **JSON 格式**寫入一個日誌檔案中。
    *   檔案命名：建議包含時間戳和唯一的管線執行ID，例如 `pipeline_run_YYYYMMDD_HHMMSS_EXECUTIONID.log`。存放於 `MyTaifexDataProject/logs/` (或 `MyTaifexDataProject/99_logs/`)。
*   **JSON 記錄結構**：每條日誌記錄應至少包含以下標準欄位：
    *   `timestamp` (字串)：精確到毫秒的 ISO 8601 格式時間戳 (使用台北時區 `Asia/Taipei`)。
    *   `execution_id` (字串)：本次管線運行的唯一ID (例如 UUID)，方便篩選和追蹤單次執行的所有相關日誌。
    *   `level` (字串)：日誌級別，如 `INFO`, `WARNING`, `ERROR`, `CRITICAL`, `DEBUG`。
    *   `module` (字串)：產生事件的程式模組名稱（例如 `IngestionPipeline`, `TransformationWorker`, `FormatDetector`, `SpecificCleanerXYZ`）。
    *   `file_hash` (字串, 可選)：若日誌事件與特定檔案相關，記錄該檔案的 SHA256 雜湊值。
    *   `message` (字串)：具體的日誌訊息內容。
    *   `extra_info` (物件, 可選)：其他與該日誌事件相關的上下文資訊（例如，錯誤堆疊追蹤、特定參數值等）。
*   **優勢**：結構化的 JSON 日誌易於被日誌管理系統（如 ELK Stack, Splunk）或數據分析工具解析和查詢，進行深入的效能分析、錯誤診斷和趨勢監控。

## 五、專案結構與程式碼風格 (Project Structure & Code Style)

### 5.1 專案結構 (Project Structure)

*   **核心原則**：「**關注點分離 (Separation of Concerns)**」與「**模組化 (Modularity)**」。
*   **建議的目錄結構**：

    ```
    MyTaifexDataProject/
    ├── config/                     # 設定檔目錄
    │   └── format_catalog.json     # 唯一的「格式指紋目錄」設定檔
    ├── data/                       # 數據相關目錄 (建議不納入 Git 版本控制，或使用 .gitignore)
    │   ├── 00_landing_zone/        # 供 API/爬蟲等外部數據源落地的原始檔案
    │   ├── 01_raw_lake/            # 存放 raw_lake.db (原始二進位檔案內容資料庫)
    │   └── 02_processed/           # 存放最終處理完成的時間序列資料庫 (例如 DuckDB 檔案)
    │   └── 03_quarantine/          # (可選) 存放轉換失敗或無法識別而被隔離的原始檔案副本
    ├── logs/                       # 存放結構化日誌檔案
    ├── notebooks/                  # 供使用者進行探索性數據分析的 Colab/Jupyter 筆記本
    ├── src/                        # 專案的核心 Python 原始碼
    │   └── taifex_pipeline/        # 主要的 Python 套件 (Package)
    │       ├── __init__.py
    │       ├── core/               # 核心共用模組 (如日誌設定、設定檔讀取器、通用工具函式)
    │       ├── ingestion/          # 「汲取階段」相關邏輯 (掃描、入庫 raw_lake.db)
    │       ├── transformation/     # 「轉換階段」相關邏輯
    │       │   ├── format_detector.py # 格式指紋計算與識別
    │       │   ├── parsers.py         # 各類檔案的 Pandas 解析邏輯 (配合 parser_config)
    │       │   └── cleaners/          # 清洗函式模組 (每個 cleaner_function 可為獨立 .py 或組織在此目錄下)
    │       ├── database/           # 資料庫互動邏輯 (連接、查詢、載入 raw_lake.db, manifest.db, processed_db)
    │       └── collectors/         # (可選) 外部數據源採集器模組
    │       └── main_pipeline.py    # 協調汲取與轉換管線的主流程控制
    ├── tests/                      # 所有單元測試與整合測試程式碼
    │   ├── unit/
    │   └── integration/
    ├── run.py                      # 整個數據管道的主啟動腳本 (命令行介面入口)
    ├── pyproject.toml              # Python 專案依賴管理與建置設定 (例如使用 Poetry 或 PDM)
    ├── setup.cfg                   # (可選) 部分工具的設定檔
    └── README.md                   # 專案說明檔案 (繁體中文)
    ```

### 5.2 程式碼風格與品質 (Code Style & Quality)

*   **核心原則**：「**一致性、可讀性、可驗證性**」。
*   **建議的工具鏈**：
    1.  **程式碼格式化 - `black`**: 自動統一程式碼風格。
    2.  **程式碼檢查 (Linting) - `Ruff`**: 快速的現代化 Linter，檢查語法錯誤和風格問題，並能自動修復部分問題。
    3.  **類型檢查 (Type Checking) - `mypy`**: 在程式碼中全面使用 Python 類型提示 (Type Hinting)，並使用 `mypy` 進行靜態類型檢查，以在執行前捕獲類型不匹配的錯誤。
*   **自動化實踐 - `pre-commit`**:
    *   設定 `pre-commit` 掛鉤 (hooks)，在每次執行 `git commit` 時自動運行 `black`、`Ruff` (包含 autofix) 和 `mypy`。
    *   只有當所有檢查都通過時，才允許提交程式碼。這能確保流入版本控制系統的每一行程式碼都符合預設的品質標準。
*   **語言**: 所有程式碼註解、日誌訊息、文檔優先使用**繁體中文**，專有名詞和無法避免的技術術語可使用英文。

## 六、最終交付資料庫 (Processed Database)

*   雖然未深入討論，但轉換後的數據最終會存入一個或多個時間序列資料庫 (TSDB) 或分析型資料庫。
*   **建議使用 DuckDB** 作為 `02_processed/` 中的最終資料庫。DuckDB 是一個高效能的內嵌式分析資料庫，非常適合處理和查詢表格數據，且與 Python/Pandas 生態整合良好。其單檔案特性也便於管理。
*   資料表結構需根據實際數據類型（如每日行情、三大法人、PCR等）分別設計，確保欄位類型正確、索引適當。

---
本文件總結了專案開發的核心設計決策與細節，將作為後續實施階段的指導藍圖。
所有內容均以繁體中文撰寫，專有名詞保留英文以確保精確性。
分支名稱: main_v3.0.1
文件最後更新時間: (Jules 自動填寫)
