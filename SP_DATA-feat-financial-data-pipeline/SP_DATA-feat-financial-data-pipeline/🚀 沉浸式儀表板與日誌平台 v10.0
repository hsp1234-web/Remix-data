# -*- coding: utf-8 -*-
# @title ğŸš€ æ²‰æµ¸å¼å„€è¡¨æ¿èˆ‡æ—¥èªŒå¹³å° v10.0 (æœ€çµ‚é«”é©—ç‰ˆ)
# @markdown ### ç³»çµ±ä»‹ç´¹
# @markdown é€™æ˜¯ä¸€å€‹ç‚ºæ‚¨é‡èº«æ‰“é€ çš„çµ‚æ¥µæ¸¬è©¦å¹³å°ã€‚å®ƒé è¨­ä»¥ä¸€å€‹**ç©©å®šã€ç„¡é–ƒçˆã€ä¸”åœ¨ä»»å‹™çµæŸå¾Œæœƒç•™å­˜çš„ä¸­æ–‡å„€è¡¨æ¿**æä¾›æ¸…çˆ½çš„ç”¨æˆ¶é«”é©—ã€‚æ‰€æœ‰ä¾†è‡ªæ‚¨å°ˆæ¡ˆçš„å…§éƒ¨æ—¥èªŒåœ¨å„€è¡¨æ¿æ¨¡å¼ä¸‹å°‡è¢«**å¾¹åº•éœéŸ³**ï¼Œä¸¦èˆ‡æ•ˆèƒ½æ•¸æ“šä¸€åŒæ•´ç†æˆ**ä¸‰ä»½æ˜“æ–¼é–±è®€çš„ä¸­æ–‡ `.txt` å ±å‘Š**ã€‚
# @markdown - **æ²‰æµ¸å¼å„€è¡¨æ¿**: æ¡ç”¨ `update_display` æŠ€è¡“ï¼Œä¸¦åœ¨ä»»å‹™çµæŸå¾Œç•™å­˜æœ€çµ‚ç‹€æ…‹ã€‚
# @markdown - **æ—¥èªŒéœé»˜æŠ€è¡“**: é€éç¨‹å¼åŒ–çµ„æ…‹ï¼Œå¾¹åº•é—œé–‰å°ˆæ¡ˆå…§éƒ¨æ—¥èªŒåœ¨å„€è¡¨æ¿æ¨¡å¼ä¸‹çš„è¢å¹•è¼¸å‡ºã€‚
# @markdown - **ä¸‰åˆä¸€ä¸­æ–‡å ±å‘Š**: è‡ªå‹•ç”Ÿæˆã€Œç¶œåˆåŸ·è¡Œå ±å‘Šã€ã€ã€Œè©³ç´°æ•ˆèƒ½ç´€éŒ„ã€ä»¥åŠåŒ…å«å…©è€…æ‰€æœ‰å…§å®¹çš„ã€Œå®Œæ•´åŸ·è¡Œç¸½è¦½ã€ä¸‰ä»½ `.txt` æª”æ¡ˆã€‚
# @markdown ---
# @markdown ### v10.0 é–‹ç™¼æ—¥èªŒ (é«”é©—çµ‚æ¥µç‰ˆ)
# @markdown - **ä¿®æ­£**: è§£æ±ºäº† `log_header` çš„ `TypeError` éŒ¯èª¤ã€‚
# @markdown - **æ–°å¢**: å„€è¡¨æ¿åœ¨ä»»å‹™çµæŸå¾Œæœƒæ›´æ–°è‡³ã€Œå®Œæˆã€ç‹€æ…‹ä¸¦ç•™å­˜ï¼Œä¸å†æ¶ˆå¤±ã€‚
# @markdown - **æ–°å¢**: å¯¦ç¾äº†æ—¥èªŒéœé»˜æŠ€è¡“ï¼Œåœ¨å„€è¡¨æ¿æ¨¡å¼ä¸‹å¾¹åº•éš±è—å°ˆæ¡ˆå…§éƒ¨æ—¥èªŒã€‚
# @markdown - **å„ªåŒ–**: é€²ä¸€æ­¥ç°¡åŒ–äº†å„€è¡¨æ¿æ¨¡å¼ä¸‹çš„æœ€çµ‚è¼¸å‡ºï¼Œé”åˆ°çµ•å°ç´”æ·¨ã€‚

# ==============================================================================
# @markdown ### æ­¥é©Ÿ 1: ğŸ“‚ è¨­å®šåŸ·è¡Œåƒæ•¸
# @markdown è«‹åœ¨æ­¤è™•è¼¸å…¥æ‚¨çš„éƒ¨ç½²ç›®æ¨™ï¼Œä¸¦é¸æ“‡æ‚¨åå¥½çš„è¼¸å‡ºæ¨¡å¼ã€‚
# ==============================================================================
# --- Git éƒ¨ç½²ç›¸é—œè¨­å®š ---
GITHUB_REPO_URL = "https://github.com/hsp1234-web/SP_DATA.git" #@param {type:"string"}
BRANCH_NAME = "feat/metadata-scanner-integration" #@param {type:"string"}
LOCAL_CLONE_PATH = "/content/deployed_project" #@param {type:"string"}

# --- æ•¸æ“šæ¢å‹˜ç›¸é—œè¨­å®š ---
GDRIVE_TARGET_FOLDER = "MyTaifexDataProject" #@param {type:"string"}

# --- ä»‹é¢èˆ‡æ—¥èªŒè¨­å®š ---
SHOW_DETAILED_LOGS = False #@param {type:"boolean"}

# ==============================================================================
# @markdown ### æ­¥é©Ÿ 2: â–¶ï¸ é»æ“ŠåŸ·è¡Œæ­¤å„²å­˜æ ¼
# @markdown è…³æœ¬å°‡å…¨è‡ªå‹•åŸ·è¡Œæ‰€æœ‰ä»»å‹™ã€‚
# ==============================================================================

# --- æ ¸å¿ƒå‡½å¼åº«å°å…¥ ---
import os
import sys
import subprocess
import shutil
import io
import time
import logging
import traceback
from datetime import datetime, timedelta
from collections import deque
from typing import List, Dict, Any
from pathlib import Path
import html
from contextlib import contextmanager

# --- Colab/IPython å°ˆç”¨å‡½å¼åº« ---
try:
    from google.colab import drive
    from IPython.display import display, HTML
    IS_COLAB = True
except ImportError:
    IS_COLAB = False
    display, HTML = print, lambda x: x

# --- ç¬¬ä¸‰æ–¹å‡½å¼åº«èˆ‡è‡ªå‹•å®‰è£ ---
try:
    import psutil
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "psutil"])
    import psutil
try:
    import pytz
except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "pytz"])
    import pytz

# --- å…¨åŸŸè¨­å®šèˆ‡è¼”åŠ©å‡½å¼ ---
TAIPEI_TZ = pytz.timezone('Asia/Taipei')
def get_taipei_time_str(ts=None, fmt='%Y-%m-%d %H:%M:%S') -> str:
    dt = datetime.fromtimestamp(ts) if ts else datetime.now()
    return dt.astimezone(TAIPEI_TZ).strftime(fmt)

def format_timedelta(seconds: float) -> str:
    return str(timedelta(seconds=int(seconds)))

def human_readable_size(size_bytes: int) -> str:
    if size_bytes is None: return "N/A"
    if size_bytes == 0: return "0 B"
    size_name = ("B", "KB", "MB", "GB", "TB")
    i = int(size_bytes.bit_length() / 10)
    p = 1024 ** i
    s = round(size_bytes / p, 2)
    return f"{s} {size_name[i]}"

# --- ç¡¬é«”ç®¡ç† ---
class HardwareManager:
    @staticmethod
    def get_status_snapshot() -> dict:
        return {'cpu': psutil.cpu_percent(), 'ram': psutil.virtual_memory().percent, 'ts': time.time()}

# --- v10.0 äº’å‹•å¼å„€è¡¨æ¿èˆ‡æ—¥èªŒç³»çµ± ---
class LogAndReportManager:
    def __init__(self, run_id: str):
        self.run_id = run_id
        self.log_dir = Path(f"é‹è¡Œæ—¥èªŒ_{self.run_id}")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        self.summary_log_path = self.log_dir / f"ç¶œåˆåŸ·è¡Œå ±å‘Š_{self.run_id}.txt"
        self.perf_log_path = self.log_dir / f"è©³ç´°æ•ˆèƒ½ç´€éŒ„_{self.run_id}.txt"
        self.combined_log_path = self.log_dir / f"å®Œæ•´åŸ·è¡Œç¸½è¦½_{self.run_id}.txt"
        self.internal_log_path = self.log_dir / f"å°ˆæ¡ˆå…§éƒ¨æ—¥èªŒ_{self.run_id}.txt"
        
        self.dashboard_handle = None

    def _get_timestamp(self) -> str:
        return get_taipei_time_str(fmt='%Y-%m-%d %H:%M:%S.%f')[:-3]
    
    def log_message(self, msg: str, level: str = "INFO", to_ui: bool = True):
        icon_map = {"INFO": "âšª", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ"}
        if to_ui:
            color_map = {"INFO": "#FFFFFF", "SUCCESS": "#4CAF50", "WARNING": "#FFC107", "ERROR": "#F44336"}
            weight = "bold" if level in ["SUCCESS", "ERROR", "WARNING"] else "normal"
            if IS_COLAB: display(HTML(f'<div style="font-family:monospace; white-space:pre-wrap; margin:0; padding:1px 0;"><span style="color:#9E9E9E; margin-right:10px;">{self._get_timestamp()}</span><span style="color:{color_map.get(level, "#FFFFFF")}; font-weight:{weight};">{icon_map.get(level, "âšª")} {html.escape(msg)}</span></div>'))
            else: print(f"{self._get_timestamp()} [{level}] {msg}")
        
        with open(self.summary_log_path, 'a', encoding='utf-8') as f:
            f.write(f"{self._get_timestamp()} [{level.ljust(7)}] {msg}\n")

    def log_header(self, msg: str, to_ui: bool = True):
        if to_ui:
            if IS_COLAB: display(HTML(f'<h3 style="color:white; border-bottom:2px solid #64B5F6; padding-bottom:5px; font-family:sans-serif; margin-top:1em;">{msg}</h3>'))
            else: print(f"\n{'='*80}\n=== {msg.strip()} ===\n{'='*80}")
        with open(self.summary_log_path, 'a', encoding='utf-8') as f:
            f.write(f"\n{'='*80}\n=== {msg.strip()} ===\n{'='*80}\n")
    
    def log_performance_event(self, event_name: str, file_path: str, snapshot: dict):
        cpu, ram = snapshot['cpu'], snapshot['ram']
        with open(self.perf_log_path, 'a', encoding='utf-8') as f:
            f.write(f"[{self._get_timestamp()}] [{event_name:13s}] CPU: {cpu:5.1f}% | RAM: {ram:5.1f}% | æª”æ¡ˆ: {file_path}\n")

    def display_diagnostic_card(self, result: Dict[str, Any]):
        status, descriptor, file_size = result.get('status', 'failure'), result.get('descriptor', 'æœªçŸ¥æª”æ¡ˆ'), result.get('size', 0)
        header_color, status_icon = ("#4CAF50", "âœ…") if status == 'success' else ("#F44336", "âŒ")
        
        header_html = f'<div style="background-color:{header_color}; color:white; padding: 8px 12px; font-family:monospace; font-weight:bold; border-radius: 5px 5px 0 0;">{status_icon} {status.upper()}: {html.escape(descriptor)}</div>'
        details_html = '<div style="background-color:#2E2E2E; padding: 12px; font-family:monospace; border-radius: 0 0 5px 5px; margin-bottom:1em; font-size:13px;">'
        
        if 'snap_before' in result and 'snap_after' in result:
            sb, sa = result['snap_before'], result['snap_after']
            td, cd, rd = sa['ts'] - sb['ts'], sa['cpu'] - sb['cpu'], sa['ram'] - sb['ram']
            details_html += f'<strong style="color:#81D4FA;">[æ•ˆèƒ½è¶³è·¡]</strong><br><span>è™•ç†è€—æ™‚: {td:.3f} ç§’</span> | <span>CPUæˆæœ¬: <span style="color:{"#FF8A80" if cd > 0 else "#B9F6CA"};">{"+" if cd >= 0 else ""}{cd:.1f}%</span></span> | <span>RAMæˆæœ¬: <span style="color:{"#FF8A80" if rd > 0 else "#B9F6CA"};">{"+" if rd >= 0 else ""}{rd:.1f}%</span></span><hr style="border-color:#444; margin: 8px 0;">'
        
        details_html += f'<strong style="color:#FFD180;">[æª”æ¡ˆè©³æƒ…]</strong><br><span>æª”æ¡ˆå¤§å°: {human_readable_size(file_size)}</span><br>'
        
        if status == 'failure':
            error_reason = result.get('error_reason', 'æœªçŸ¥éŒ¯èª¤')
            details_html += f'<br><strong style="color:#FF8A80;">éŒ¯èª¤è¿½è¹¤ (TRACEBACK):</strong><pre style="white-space:pre-wrap; margin:0; font-family:monospace; color:#FFCDD2; background-color:#333; padding: 8px; border-radius: 4px;">{html.escape(error_reason)}</pre>'
        
        details_html += '</div>'
        if IS_COLAB: display(HTML(header_html + details_html))
        else: print(f"\n--- Report for {descriptor} ---\nStatus: {status.upper()}")

    def init_dashboard(self):
        if IS_COLAB: self.dashboard_handle = display(HTML(""), display_id=True)

    def update_dashboard(self, p_count: int, t_count: int, q_size: int, file: str, elapsed: float, completed: bool = False):
        if IS_COLAB and self.dashboard_handle:
            percentage = (p_count / t_count) * 100 if t_count > 0 else 100
            bar_len = 30
            filled_len = int(bar_len * p_count / t_count) if t_count > 0 else bar_len
            
            if completed:
                bar = 'â–°' * bar_len
                status_color = "#76D7C4"
                title = "âœ… ç®¡ç·šåŸ·è¡Œå®Œç•¢ï¼"
                status_text = f"æ‰€æœ‰ {p_count} å€‹ä»»å‹™å·²æˆåŠŸè™•ç†ã€‚"
            else:
                bar = 'â–°' * filled_len + 'â–±' * (bar_len - filled_len)
                status_color = "#64B5F6"
                title = "ğŸš€ è³‡æ–™æ“·å–ç®¡ç·šåŸ·è¡Œä¸­..."
                status_text = f"æ­£åœ¨è™•ç†: {html.escape(file)}"

            dashboard_html = f"""
            <div style="font-family: 'SF Mono', 'Consolas', monospace; background-color: #2E2E2E; padding: 18px; border-radius: 10px; color: white; line-height: 1.8;">
                <div style="font-size: 1.3em; margin-bottom: 12px; font-weight: bold; color: {status_color};">{title}</div>
                <div style="font-size:1.1em; color: #F7DC6F; margin-bottom: 8px;"><b>[ {bar} ] {percentage:.1f}%</b></div>
                <div>
                  <span style="color:#ABB2B9;">é€²åº¦:</span> <b style="color:white; font-size:1.1em;">{p_count} / {t_count}</b> &nbsp;|&nbsp; 
                  <span style="color:#ABB2B9;">ä½‡åˆ—:</span> <b style="color:white;">{q_size}</b> &nbsp;|&nbsp;
                  <span style="color:#ABB2B9;">é‹è¡Œæ™‚é–“:</span> <b style="color:white;">{format_timedelta(elapsed)}</b>
                </div>
                <div style="margin-top: 10px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;">
                    <span style="color:#ABB2B9;">ç‹€æ…‹:</span> <span style="color:#E5E7E9;">{status_text}</span>
                </div>
            </div>
            """
            self.dashboard_handle.update(HTML(dashboard_html))

    def finalize_reports(self, all_reports: List[Dict], duration: float, output_dir: Path, internal_logs: str):
        with open(self.internal_log_path, 'w', encoding='utf-8') as f: f.write(internal_logs)
        
        self.log_header("ç”¢å‡ºæœ€çµ‚å ±å‘Š", to_ui=False)
        summary_lines, successful_tasks, failed_tasks = [], [r for r in all_reports if r['status'] == 'success'], [r for r in all_reports if r['status'] == 'failure']

        summary_lines.extend(["="*40, "          ç¶œåˆåŸ·è¡Œå ±å‘Š", "="*40,
            f"å ±å‘Šç”Ÿæˆæ™‚é–“: {get_taipei_time_str()}", f"ç¸½åŸ·è¡Œè€—æ™‚: {format_timedelta(duration)}", "-"*40,
            "\nã€ä»»å‹™ç¸½è¦½ã€‘", f"  - ç¸½å…±è™•ç†å£“ç¸®æª”æ•¸: {len(all_reports)}", f"  - âœ… æˆåŠŸ: {len(successful_tasks)}", f"  - âŒ å¤±æ•—: {len(failed_tasks)}"])
        
        if failed_tasks:
            summary_lines.append("\nã€å¤±æ•—æª”æ¡ˆè©³æƒ…ã€‘")
            for i, task in enumerate(failed_tasks, 1):
                summary_lines.extend([f"\n--- {i}. {task['descriptor']} ---", "  éŒ¯èª¤è¿½è¹¤ (Traceback):", task.get('error_reason', 'æœªçŸ¥éŒ¯èª¤')])
        
        if successful_tasks:
            summary_lines.append("\nã€æ•ˆèƒ½ç“¶é ¸åˆ†æ (è™•ç†è€—æ™‚å‰ 5 å)ã€‘")
            sorted_by_time = sorted(successful_tasks, key=lambda r: (r['snap_after']['ts'] - r['snap_before']['ts']), reverse=True)
            for i, task in enumerate(sorted_by_time[:5], 1):
                summary_lines.append(f"  {i}. {(task['snap_after']['ts'] - task['snap_before']['ts']):.3f} ç§’ - {task['descriptor']}")

        extracted_files = [str(p.relative_to(output_dir)) for p in output_dir.glob("**/*") if p.is_file()]
        summary_lines.extend(["\nã€æœ€çµ‚ç”¢å‡ºæª”æ¡ˆæ¸…å–®ã€‘", f"  - å…±æå–äº† {len(extracted_files)} å€‹æª”æ¡ˆã€‚"])
        summary_lines.extend([f"    - {f}" for f in sorted(extracted_files)])
        
        with open(self.summary_log_path, 'w', encoding='utf-8') as f: f.write("\n".join(summary_lines))
        
        with open(self.combined_log_path, 'w', encoding='utf-8') as f_comb:
            f_comb.write("\n".join(summary_lines))
            f_comb.write("\n\n" + "="*80 + "\n          è©³ç´°æ•ˆèƒ½ç´€éŒ„\n" + "="*80 + "\n\n")
            with open(self.perf_log_path, 'r', encoding='utf-8') as f_perf: f_comb.write(f_perf.read())
            f_comb.write("\n\n" + "="*80 + "\n          å°ˆæ¡ˆå…§éƒ¨æ—¥èªŒ (æ“·å–è‡ª Console)\n" + "="*80 + "\n\n")
            f_comb.write(internal_logs)

@contextmanager
def suppress_project_logs(should_suppress: bool, internal_log_stream: io.StringIO):
    """ä¸€å€‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨æ–¼åœ¨å„€è¡¨æ¿æ¨¡å¼ä¸‹æš«æ™‚éœéŸ³å°ˆæ¡ˆçš„æ ¹æ—¥èªŒè¨˜éŒ„å™¨"""
    if not should_suppress:
        yield
        return
        
    project_logger = logging.getLogger('taifex_pipeline')
    if not project_logger:
        yield
        return
        
    original_handlers = project_logger.handlers[:]
    
    # å‰µå»ºä¸€å€‹å°ˆé–€ç”¨æ–¼æ•ç²æ—¥èªŒçš„ handler
    capture_handler = logging.StreamHandler(internal_log_stream)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)')
    capture_handler.setFormatter(formatter)
    
    # ç§»é™¤æ‰€æœ‰ç¾æœ‰çš„ handlerï¼Œä¸¦æ›ä¸Šæˆ‘å€‘çš„ capture_handler
    project_logger.handlers = [capture_handler]
    
    try:
        yield
    finally:
        # æ¢å¾©åŸå§‹çš„ handlers
        project_logger.handlers = original_handlers

# --- ä¸»åŸ·è¡Œæµç¨‹ ---
def main():
    run_id = get_taipei_time_str(fmt='%Y%m%d_%H%M%S')
    log_manager = LogAndReportManager(run_id)

    try:
        log_manager.log_header("ğŸš€ æ²‰æµ¸å¼å„€è¡¨æ¿èˆ‡æ—¥èªŒå¹³å° v10.0 å•Ÿå‹•")
        log_manager.log_header("éšæ®µä¸€: Git å°ˆæ¡ˆéƒ¨ç½²èˆ‡ç’°å¢ƒå»ºç½®")
        
        if os.path.exists(LOCAL_CLONE_PATH): shutil.rmtree(LOCAL_CLONE_PATH)
        git_command = ["git", "clone", "--branch", BRANCH_NAME, "--single-branch", GITHUB_REPO_URL, LOCAL_CLONE_PATH]
        log_manager.log_message(f"åŸ·è¡ŒæŒ‡ä»¤: {' '.join(git_command)}")
        git_result = subprocess.run(git_command, capture_output=True, text=True, encoding='utf-8', check=False)
        if git_result.returncode != 0:
            log_manager.log_message(f"Git clone å¤±æ•—ï¼\n{git_result.stderr.strip()}", "ERROR")
            return
        log_manager.log_message("å°ˆæ¡ˆåˆ†æ”¯æˆåŠŸæ‹‰å–ï¼", "SUCCESS")
        
        requirements_path = os.path.join(LOCAL_CLONE_PATH, 'requirements.txt')
        if os.path.exists(requirements_path):
            log_manager.log_message("æ­£åœ¨å®‰è£é€šç”¨ä¾è³´...")
            pip_command = [sys.executable, "-m", "pip", "install", "-q", "-r", requirements_path]
            pip_result = subprocess.run(pip_command, capture_output=True, text=True, encoding='utf-8')
            if pip_result.returncode != 0:
                log_manager.log_message(f"é€šç”¨ä¾è³´å®‰è£å¯èƒ½ä¸å®Œæ•´ã€‚\néŒ¯èª¤è¨Šæ¯: {pip_result.stderr.strip()}", "WARNING")
            else:
                log_manager.log_message("é€šç”¨ä¾è³´å®‰è£å®Œæˆï¼", "SUCCESS")

        project_src_path = os.path.join(LOCAL_CLONE_PATH, 'src')
        if not os.path.isdir(project_src_path): project_src_path = LOCAL_CLONE_PATH
        sys.path.insert(0, project_src_path)
        log_manager.log_message(f"å·²å°‡å°ˆæ¡ˆåŸå§‹ç¢¼è·¯å¾‘ '{project_src_path}' åŠ å…¥åˆ°ç³»çµ±è·¯å¾‘ã€‚")

        from taifex_pipeline.ingestion.pipeline import IngestionPipeline

    except Exception:
        log_manager.log_message(f"éšæ®µä¸€ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {traceback.format_exc()}", "ERROR")
        return

    start_time = time.time()
    raw_data_output_path = Path(f"{LOCAL_CLONE_PATH}/data/01_raw")
    all_final_reports = []
    
    try:
        log_manager.log_header("éšæ®µäºŒ: å¾ªåºä»»å‹™èª¿åº¦èˆ‡åŸ·è¡Œ", to_ui=SHOW_DETAILED_LOGS)
        drive.mount('/content/drive', force_remount=True, timeout_ms=60000)
        full_target_path = Path("/content/drive/MyDrive") / GDRIVE_TARGET_FOLDER
        if not full_target_path.is_dir():
            log_manager.log_message(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ•¸æ“šä¾†æºè·¯å¾‘ '{full_target_path}'ã€‚", "ERROR")
            return
        log_manager.log_message(f"Google Drive æ›è¼‰æˆåŠŸï¼Œä¾†æº: {full_target_path}", "SUCCESS", to_ui=SHOW_DETAILED_LOGS)

        if raw_data_output_path.exists(): shutil.rmtree(raw_data_output_path)
        raw_data_output_path.mkdir(parents=True, exist_ok=True)
        log_manager.log_message(f"å·²æº–å‚™å¥½è¼¸å‡ºç›®éŒ„: {raw_data_output_path}", to_ui=SHOW_DETAILED_LOGS)

        pipeline_instance = IngestionPipeline(config={"ingestion": {"output_dir": str(raw_data_output_path)}})
        
        work_queue, processed_paths = deque(), set()
        log_manager.log_message("æ­£åœ¨éè¿´æƒæä¾†æºç›®éŒ„ä»¥å°‹æ‰¾æ‰€æœ‰ .zip æª”æ¡ˆ...", to_ui=SHOW_DETAILED_LOGS)
        all_zip_files = list(full_target_path.rglob("*.zip"))
        work_queue.extend(all_zip_files)
        total_tasks, processed_count = len(all_zip_files), 0
        log_manager.log_message(f"æƒæå®Œæˆï¼Œç™¼ç¾ {total_tasks} å€‹å£“ç¸®æª”ä»»å‹™ã€‚", to_ui=SHOW_DETAILED_LOGS)
        
        if not work_queue:
            log_manager.log_message("åœ¨ç›®æ¨™ç›®éŒ„åŠå…¶æ‰€æœ‰å­ç›®éŒ„ä¸­å‡æœªç™¼ç¾ .zip æª”æ¡ˆã€‚æµç¨‹çµæŸã€‚", "WARNING")
            return
        
        if not SHOW_DETAILED_LOGS: log_manager.init_dashboard()
        else: log_manager.log_message("é–‹å§‹å¾ªåºè™•ç†ä»»å‹™...")

        internal_log_capture = io.StringIO()

        with suppress_project_logs(should_suppress=not SHOW_DETAILED_LOGS, internal_log_stream=internal_log_capture):
            while work_queue:
                zip_path = work_queue.popleft()
                if zip_path in processed_paths:
                    log_manager.log_message(f"åµæ¸¬åˆ°å¾ªç’°ä¾è³´ï¼Œè·³é: {zip_path.name}", "WARNING", to_ui=SHOW_DETAILED_LOGS)
                    continue
                processed_paths.add(zip_path)
                
                try: display_path_str = str(zip_path.relative_to(full_target_path.parent))
                except ValueError: display_path_str = f"(å·¢ç‹€) {zip_path.name}"
                
                if not SHOW_DETAILED_LOGS:
                    log_manager.update_dashboard(processed_count, total_tasks, len(work_queue), display_path_str, time.time() - start_time)

                snap_before = HardwareManager.get_status_snapshot()
                log_manager.log_performance_event("process_start", str(zip_path), snap_before)
                report = {}
                
                try:
                    nested_zips = pipeline_instance._process_single_zip(zip_path)
                    snap_after = HardwareManager.get_status_snapshot()
                    report = {"status": "success", "descriptor": str(zip_path), "size": zip_path.stat().st_size, "snap_before": snap_before, "snap_after": snap_after}
                    
                    for nested_zip_path in nested_zips:
                        if nested_zip_path not in processed_paths:
                            work_queue.append(nested_zip_path)
                            total_tasks += 1
                except Exception:
                    snap_after = HardwareManager.get_status_snapshot()
                    report = {"status": "failure", "descriptor": str(zip_path), "size": zip_path.stat().st_size if zip_path.exists() else 0, "error_reason": traceback.format_exc(), "snap_before": snap_before, "snap_after": snap_after}
                
                log_manager.log_performance_event("process_end", str(zip_path), snap_after)
                
                if SHOW_DETAILED_LOGS or report["status"] == "failure":
                    if not SHOW_DETAILED_LOGS and report["status"] == "failure":
                        log_manager.update_dashboard(processed_count, total_tasks, len(work_queue), display_path_str, time.time() - start_time, completed=False)
                    log_manager.display_diagnostic_card(report)

                all_final_reports.append(report)
                processed_count += 1
        
        duration = time.time() - start_time
        if not SHOW_DETAILED_LOGS:
            log_manager.update_dashboard(processed_count, total_tasks, len(work_queue), "", duration, completed=True)
        
        log_manager.log_message(f"æ‰€æœ‰ä»»å‹™è™•ç†å®Œç•¢ï¼Œç¸½è€—æ™‚ {format_timedelta(duration)}ã€‚", "SUCCESS", to_ui=SHOW_DETAILED_LOGS)
        
        log_manager.finalize_reports(all_final_reports, duration, raw_data_output_path, internal_log_capture.getvalue())
        
    except Exception:
        log_manager.log_message(f"éšæ®µäºŒç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {traceback.format_exc()}", "ERROR")
    finally:
        log_manager.log_header("ğŸ“‹ å¹³å°åŸ·è¡ŒçµæŸ", to_ui=SHOW_DETAILED_LOGS)
        log_manager.log_message(f"æ‰€æœ‰æ—¥èªŒå·²ä¿å­˜è‡³ '{log_manager.log_dir}' è³‡æ–™å¤¾ã€‚", "INFO")

if __name__ == '__main__':
    main()
