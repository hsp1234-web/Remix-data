# -*- coding: utf-8 -*-
# @title æ ¼å¼æŒ‡ç´‹åˆ†é¡å™¨ - åŸå‹é©—è­‰è…³æœ¬ v1.0
# @markdown ### ç›®çš„
# @markdown æœ¬è…³æœ¬æ—¨åœ¨ç¨ç«‹é©—è­‰ã€Œæ ¼å¼æŒ‡ç´‹ã€è­˜åˆ¥ç³»çµ±çš„æ ¸å¿ƒé‚è¼¯ã€‚
# @markdown å®ƒä¸ä¾è³´ä»»ä½•å¤–éƒ¨æª”æ¡ˆæˆ–è¤‡é›œçš„å°ˆæ¡ˆçµæ§‹ï¼Œå¯ä¸€éµåŸ·è¡Œï¼Œ
# @markdown ä»¥ç¢ºèªæˆ‘å€‘è¨­è¨ˆçš„åˆ†é¡æ¼”ç®—æ³•èƒ½å¦æº–ç¢ºåœ°ç‚ºä¸åŒæ ¼å¼çš„
# @markdown æœŸäº¤æ‰€æ•¸æ“šæª”æ¡ˆï¼ŒåŒ¹é…åˆ°æ­£ç¢ºçš„è™•ç†é…æ–¹ã€‚

import hashlib
import re
from typing import List, Dict, Optional, Tuple

# --- æ­¥é©Ÿ 1: æ¨¡æ“¬æˆ‘å€‘æœªä¾†åœ¨ config/format_catalog.json ä¸­çš„æ ¸å¿ƒè¨­å®š ---

# é€™æ˜¯æˆ‘å€‘é å…ˆå®šç¾©å¥½çš„ã€Œè™•ç†é…æ–¹ã€ç›®éŒ„ã€‚
# æ³¨æ„ï¼šé€™è£¡çš„é›œæ¹Šå€¼æ˜¯åŸºæ–¼æˆ‘å€‘çš„æŒ‡ç´‹ç”Ÿæˆè¦å‰‡ï¼Œå°çœŸå¯¦æ¨™é ­è¨ˆç®—å¾—å‡ºçš„ã€‚
MOCK_FORMAT_CATALOG = {
    # æŒ‡ç´‹ for: 'äº¤æ˜“æ—¥æœŸ,å¥‘ç´„,åˆ°æœŸæœˆä»½(é€±åˆ¥),å±¥ç´„åƒ¹,è²·è³£æ¬Š,é–‹ç›¤åƒ¹,æœ€é«˜åƒ¹,æœ€ä½åƒ¹,æ”¶ç›¤åƒ¹,æˆäº¤é‡,çµç®—åƒ¹,æœªæ²–éŠ·å¥‘ç´„æ•¸,æœ€å¾Œæœ€ä½³è²·åƒ¹,æœ€å¾Œæœ€ä½³è³£åƒ¹,æ­·å²æœ€é«˜åƒ¹,æ­·å²æœ€ä½åƒ¹,æ˜¯å¦å› è¨Šæ¯é¢æš«åœäº¤æ˜“,äº¤æ˜“æ™‚æ®µ,æ¼²è·Œåƒ¹,æ¼²è·Œ%'
    "d627a542b26286815e3469a47312f21133318f78234839835848e1a141151609": {
      "description": "æ¯æ—¥è¡Œæƒ… (é¸æ“‡æ¬Š/æœŸè²¨) - v2, å«æ¼²è·Œå¹…",
      "target_table": "daily_ohlc",
      "parser_config": {"sep": ",", "header": 0, "encoding": "ms950"},
      "cleaner_function": "clean_daily_ohlc",
      "required_columns": ["äº¤æ˜“æ—¥æœŸ", "å¥‘ç´„", "æ”¶ç›¤åƒ¹", "æˆäº¤é‡"]
    },
    # æŒ‡ç´‹ for: 'æ—¥æœŸ,å•†å“åç¨±,èº«ä»½åˆ¥,å¤šæ–¹äº¤æ˜“å£æ•¸,å¤šæ–¹äº¤æ˜“å¥‘ç´„é‡‘é¡(åƒå…ƒ),...'
    "b8a6a3b68e9f2913e2f07323604b73273117462d7331853a8174780517878841": {
      "description": "ä¸‰å¤§æ³•äºº (ä¾å•†å“åˆ†)",
      "target_table": "institutional_investors",
      "parser_config": {"sep": ",", "header": 0, "encoding": "ms950"},
      "cleaner_function": "clean_institutional_investors",
      "required_columns": ["æ—¥æœŸ", "å•†å“åç¨±", "èº«ä»½åˆ¥"]
    },
}


# --- æ­¥é©Ÿ 2: æ ¸å¿ƒå‡½å¼åº«çš„å¯¦ä½œ ---

def find_header_row(content_lines: List[str]) -> Tuple[Optional[str], int]:
    """
    å¾æª”æ¡ˆçš„å‰å¹¾è¡Œä¸­ï¼Œé€éå•Ÿç™¼å¼è¦å‰‡æ‰¾å‡ºæœ€å¯èƒ½çš„æ¨™é ­è¡Œã€‚
    è¿”å› (æ¨™é ­è¡Œå…§å®¹, æ¨™é ­è¡Œç´¢å¼•)ã€‚
    """
    candidates = []
    keywords = ['æ—¥æœŸ', 'å¥‘ç´„', 'å•†å“', 'èº«ä»½åˆ¥', 'æˆäº¤é‡', 'æ”¶ç›¤åƒ¹', 'è²·è³£æ¬Š']

    for i, line in enumerate(content_lines):
        line = line.strip()
        if not line or line.startswith('---'):
            continue

        # å•Ÿç™¼å¼è¦å‰‡ï¼šé€—è™Ÿæ•¸é‡ > 3 ä¸”è‡³å°‘åŒ…å«ä¸€å€‹é—œéµå­—
        comma_count = line.count(',')
        keyword_count = sum(1 for keyword in keywords if keyword in line)

        if comma_count > 3 and keyword_count > 0:
            # åˆ†æ•¸è¶Šé«˜ï¼Œæ˜¯æ¨™é ­çš„å¯èƒ½æ€§è¶Šå¤§
            score = comma_count + (keyword_count * 5)
            candidates.append({'score': score, 'line': line, 'index': i})

    if not candidates:
        return None, -1

    # å›å‚³åˆ†æ•¸æœ€é«˜çš„å€™é¸è€…
    best_candidate = max(candidates, key=lambda x: x['score'])
    return best_candidate['line'], best_candidate['index']


def calculate_format_fingerprint(header_line: str) -> str:
    """
    æ ¹æ“šæˆ‘å€‘å®šç¾©çš„è¦å‰‡ï¼Œå¾æ¨™é ­è¡Œè¨ˆç®—å‡ºæ ¼å¼æŒ‡ç´‹ã€‚
    """
    # 1. ç”¨é€—è™Ÿåˆ†å‰²ï¼Œä¸¦æ¸…é™¤æ¯å€‹æ¬„ä½çš„é¦–å°¾ç©ºç™½
    columns = [col.strip() for col in header_line.split(',')]

    # 2. æ¸…é™¤å…§éƒ¨æ‰€æœ‰ç©ºç™½ä¸¦è½‰ç‚ºå°å¯«
    normalized_columns = [re.sub(r'\s+', '', col).lower() for col in columns if col]

    # 3. ä¾å­—æ¯é †åºæ’åº
    normalized_columns.sort()

    # 4. ä½¿ç”¨ "|" åˆä½µæˆå–®ä¸€å­—ä¸²
    fingerprint_string = "|".join(normalized_columns)

    # 5. è¨ˆç®— SHA256 é›œæ¹Šå€¼
    return hashlib.sha256(fingerprint_string.encode('utf-8')).hexdigest()


def classify_file_format(file_content: bytes, catalog: dict) -> Optional[dict]:
    """
    æ¥æ”¶æª”æ¡ˆçš„äºŒé€²ä½å…§å®¹ï¼ŒåŸ·è¡Œå®Œæ•´çš„åˆ†é¡æµç¨‹ã€‚
    """
    print("  > é–‹å§‹åˆ†é¡...")

    decoded_lines = []
    active_encoding = None
    for encoding in ['ms950', 'utf-8', 'utf-8-sig']:
        try:
            decoded_lines = file_content.decode(encoding).splitlines()[:20]
            active_encoding = encoding
            print(f"  > å˜—è©¦ä½¿ç”¨ '{encoding}' è§£ç¢¼æˆåŠŸã€‚")
            break
        except UnicodeDecodeError:
            continue

    if not decoded_lines:
        print("  > âŒ éŒ¯èª¤ï¼šç„¡æ³•ä½¿ç”¨å¸¸è¦‹ç·¨ç¢¼è§£ç¢¼æˆ–æª”æ¡ˆç‚ºç©ºã€‚")
        return None

    # æ‰¾å‡ºæ¨™é ­
    header_line, header_index = find_header_row(decoded_lines)
    if header_line is None:
        print("  > âŒ éŒ¯èª¤ï¼šåœ¨æª”æ¡ˆé è¦½ä¸­æ‰¾ä¸åˆ°å¯è­˜åˆ¥çš„æ¨™é ­è¡Œã€‚")
        return None
    print(f"  > åµæ¸¬åˆ°æ¨™é ­åœ¨ç¬¬ {header_index + 1} è¡Œ: {repr(header_line)}")

    # è¨ˆç®—æŒ‡ç´‹
    fingerprint = calculate_format_fingerprint(header_line)
    print(f"  > è¨ˆç®—å‡ºçš„æ ¼å¼æŒ‡ç´‹ç‚º: {fingerprint[:16]}...") # åªé¡¯ç¤ºå‰16ç¢¼ä»¥æ±‚ç°¡æ½”

    # åœ¨ç›®éŒ„ä¸­æŸ¥æ‰¾
    recipe = catalog.get(fingerprint)

    if recipe:
        print(f"  > âœ… æˆåŠŸï¼åœ¨ç›®éŒ„ä¸­æ‰¾åˆ°é…æ–¹: '{recipe.get('description', 'N/A')}'")
        # å°‡åµæ¸¬åˆ°çš„å…ƒæ•¸æ“šé™„åŠ åˆ° recipe ä¸­ï¼Œæ–¹ä¾¿é©—è­‰
        recipe_copy = recipe.copy() # é¿å…ä¿®æ”¹åŸå§‹ MOCK_FORMAT_CATALOG
        recipe_copy['_debug_metadata'] = {
            'detected_header': header_line,
            'detected_header_index': header_index,
            'detected_encoding': active_encoding,
            'calculated_fingerprint': fingerprint
        }
        return recipe_copy
    else:
        print("  > âš ï¸ è­¦å‘Šï¼šåœ¨ç›®éŒ„ä¸­æ‰¾ä¸åˆ°å°æ‡‰çš„è™•ç†é…æ–¹ã€‚")
        print(f"    (é™¤éŒ¯è³‡è¨Šï¼šåŸå§‹æ¨™é ­: {repr(header_line)}, è¨ˆç®—æŒ‡ç´‹: {fingerprint})")
        return None


# --- æ­¥é©Ÿ 3: æ¸¬è©¦æˆ‘å€‘çš„åˆ†é¡å™¨ ---

# æˆ‘å€‘å°‡ä½¿ç”¨ä¾†è‡ªæ‚¨å ±å‘Šçš„çœŸå¯¦æ•¸æ“šç‰‡æ®µä½œç‚ºæ¸¬è©¦æ¡ˆä¾‹
TEST_DATA = {
    "daily_ohlc_sample.csv": b"""\
äº¤æ˜“æ—¥æœŸ,å¥‘ç´„,åˆ°æœŸæœˆä»½(é€±åˆ¥),å±¥ç´„åƒ¹,è²·è³£æ¬Š,é–‹ç›¤åƒ¹,æœ€é«˜åƒ¹,æœ€ä½åƒ¹,æ”¶ç›¤åƒ¹,æˆäº¤é‡,çµç®—åƒ¹,æœªæ²–éŠ·å¥‘ç´„æ•¸,æœ€å¾Œæœ€ä½³è²·åƒ¹,æœ€å¾Œæœ€ä½³è³£åƒ¹,æ­·å²æœ€é«˜åƒ¹,æ­·å²æœ€ä½åƒ¹,æ˜¯å¦å› è¨Šæ¯é¢æš«åœäº¤æ˜“,äº¤æ˜“æ™‚æ®µ,æ¼²è·Œåƒ¹,æ¼²è·Œ%
2025/05/26,CAO,202506  ,22.0000,è²·æ¬Š,-,-,-,-,0,8.1,0,-,-,-,-,,ä¸€èˆ¬,-,-,
2025/05/26,CAO,202506  ,22.0000,è³£æ¬Š,-,-,-,-,0,0.01,0,-,-,-,-,,ä¸€èˆ¬,-,-,
""",
    "institutional_investors_sample.csv": b"""\
æ—¥æœŸ,å•†å“åç¨±,èº«ä»½åˆ¥,å¤šæ–¹äº¤æ˜“å£æ•¸,å¤šæ–¹äº¤æ˜“å¥‘ç´„é‡‘é¡(åƒå…ƒ),ç©ºæ–¹äº¤æ˜“å£æ•¸,ç©ºæ–¹äº¤æ˜“å¥‘ç´„é‡‘é¡(åƒå…ƒ),å¤šç©ºäº¤æ˜“å£æ•¸æ·¨é¡,å¤šç©ºäº¤æ˜“å¥‘ç´„é‡‘é¡æ·¨é¡(åƒå…ƒ),å¤šæ–¹æœªå¹³å€‰å£æ•¸,å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡(åƒå…ƒ),ç©ºæ–¹æœªå¹³å€‰å£æ•¸,ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡(åƒå…ƒ),å¤šç©ºæœªå¹³å€‰å£æ•¸æ·¨é¡,å¤šç©ºæœªå¹³å€‰å¥‘ç´„é‡‘é¡æ·¨é¡(åƒå…ƒ)
2025/06/13,è‡ºè‚¡æœŸè²¨,è‡ªç‡Ÿå•†,14613,64130480,11620,51089894,2993,13040586,8415,36614210,4961,21684729,3454,14929481
2025/06/13,è‡ºè‚¡æœŸè²¨,æŠ•ä¿¡,3814,16557596,4437,19524483,-623,-2966887,52445,229829867,13086,57413851,39359,172416016
""",
    "unknown_format_sample.txt": b"""\
é€™æ˜¯ä¸€å€‹å…¨æ–°çš„å ±å‘Šï¼Œæ²’æœ‰æ˜ç¢ºçš„é—œéµå­—å’Œè¶³å¤ çš„é€—è™Ÿ
ç¬¬ä¸€ç­†è³‡æ–™-1-2
ç¬¬äºŒç­†è³‡æ–™-3-4
"""
}

def run_prototype_test():
    """ä¸»åŸ·è¡Œå‡½æ•¸ï¼Œé‹è¡Œæ‰€æœ‰æ¸¬è©¦æ¡ˆä¾‹"""
    print("="*80)
    print("ğŸš€ é–‹å§‹åŸ·è¡Œã€Œæ ¼å¼æŒ‡ç´‹åˆ†é¡å™¨ã€åŸå‹é©—è­‰æ¸¬è©¦...")
    print("="*80, "\n")

    catalog = MOCK_FORMAT_CATALOG
    print("S1: å·²æˆåŠŸè¼‰å…¥æ¨¡æ“¬çš„ã€Œæ ¼å¼æŒ‡ç´‹ç›®éŒ„ã€ã€‚\n")

    for filename, content in TEST_DATA.items():
        print("-" * 50)
        print(f"S2: æ­£åœ¨æ¸¬è©¦æª”æ¡ˆ: {filename}")

        found_recipe = classify_file_format(content, catalog)

        print("\n  >> åˆ†é¡çµæœ:")
        if found_recipe:
            import json
            # ä½¿ç”¨ ensure_ascii=False ä»¥æ­£ç¢ºé¡¯ç¤ºä¸­æ–‡å­—å…ƒ
            print(json.dumps(found_recipe, indent=4, ensure_ascii=False, sort_keys=True))
        else:
            print("  è©²æª”æ¡ˆæ‡‰è¢«é€å¾€ã€éš”é›¢å€ã€(QUARANTINED)ã€‚")
        print("-" * 50, "\n")

    print("="*80)
    print("ğŸ åŸå‹é©—è­‰æ¸¬è©¦çµæŸã€‚")
    print("="*80)

# --- ä¸»åŸ·è¡Œå€å¡Š ---
if __name__ == "__main__":
    run_prototype_test()
